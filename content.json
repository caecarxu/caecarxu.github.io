{"pages":[{"title":"","text":"","link":"404.html"},{"title":"About","text":"这个博客是为了记录我技术上的一些经历，并践行 “分享既成长” 博客中会有我的学习经历，个人随想，图片分享等内容。如需联系我,以下是我的联系方式 请发邮件：x9693xuzhenhai@163.com 这个博客还在测试中，可能会有意想不到的bug 我的其他平台： weibo: https://weibo.com/u/7612955402 知乎： https://www.zhihu.com/people/ceacer-37 github: https://github.com/caecarxu","link":"about/index.html"},{"title":"Categories","text":"","link":"categories/index.html"},{"title":"Archives","text":"","link":"archive/index.html"},{"title":"Tags","text":"","link":"tags/index.html"}],"posts":[{"title":"ArrayDeque源码学习总结","text":"源码来自与OpenJDK 17 我选择个人认为重要的点进行总结 ArrayDeque继承了AbstractCollection抽象类，实现了Deque, Cloneable, Serializable接口 12public class ArrayDeque&lt;E&gt; extends AbstractCollection&lt;E&gt; implements Deque&lt;E&gt;, Cloneable, Serializable ArrayDeque底层使用数组进行队列的存储。 1transient Object[] elements; ArrayDeque，提供public boolean add(E e) 函数进行对象的添加。 在添加时，遇到队列满的情况时，使用grow(int needed)函数进行数组的扩大。 若内存容量足够，则扩展的容量为原容量（oldCapacity）的一半，若不充足则扩充至所需最小容量。 扩容时，调用public static native void arraycopy(Object src, int srcPos,Object dest, int destPos,int length)函数进行扩容。 1234567891011121314151617181920212223242526272829303132333435public boolean add(E e) { addLast(e); return true; } public void addLast(E e) { if (e == null) throw new NullPointerException(); final Object[] es = elements; es[tail] = e; if (head == (tail = inc(tail, es.length))) grow(1); //队列满 } private void grow(int needed) { // overflow-conscious code final int oldCapacity = elements.length; int newCapacity; // Double capacity if small; else grow by 50% int jump = (oldCapacity &lt; 64) ? (oldCapacity + 2) : (oldCapacity &gt;&gt; 1); if (jump &lt; needed || (newCapacity = (oldCapacity + jump)) - MAX_ARRAY_SIZE &gt; 0) newCapacity = newCapacity(needed, jump); final Object[] es = elements = Arrays.copyOf(elements, newCapacity); // Exceptionally, here tail == head needs to be disambiguated if (tail &lt; head || (tail == head &amp;&amp; es[head] != null)) { // wrap around; slide first leg forward to end of array int newSpace = newCapacity - oldCapacity; System.arraycopy(es, head, es, head + newSpace, oldCapacity - head); for (int i = head, to = (head += newSpace); i &lt; to; i++) es[i] = null; } } 当ArrayDeque会使用 inc(int i, int modulus)、 dec(int i, int modulus) inc(int i, int distance, int modulus) sub(int i, int j, int modulus) 来实现head和tail在数组中的循环，从而实现循环队列 12345678910111213141516171819static final int inc(int i, int modulus) { if (++i &gt;= modulus) i = 0; return i;}static final int dec(int i, int modulus) { if (--i &lt; 0) i = modulus - 1; return i;}static final int inc(int i, int distance, int modulus) { if ((i += distance) - modulus &gt;= 0) i -= modulus; return i;}static final int sub(int i, int j, int modulus) { if ((i -= j) &lt; 0) i += modulus; return i;} ArrayDeque，使用remove删除特定元素 12345678910111213141516171819public boolean remove(Object o) { return removeFirstOccurrence(o);}public boolean removeFirstOccurrence(Object o) { if (o != null) { final Object[] es = elements; for (int i = head, end = tail, to = (i &lt;= end) ? end : es.length; ; i = 0, to = end) { for (; i &lt; to; i++) if (o.equals(es[i])) { delete(i); return true; } if (to == end) break; } } return false;} ArrayDeque使用delete删除index下的元素 12345678910111213141516171819202122232425262728293031323334boolean delete(int i) { final Object[] es = elements; final int capacity = es.length; final int h, t; // number of elements before to-be-deleted elt final int front = sub(i, h = head, capacity); // number of elements after to-be-deleted elt final int back = sub(t = tail, i, capacity) - 1; if (front &lt; back) { // move front elements forwards if (h &lt;= i) { System.arraycopy(es, h, es, h + 1, front); } else { // Wrap around System.arraycopy(es, 0, es, 1, i); es[0] = es[capacity - 1]; System.arraycopy(es, h, es, h + 1, front - (i + 1)); } es[h] = null; head = inc(h, capacity); return false; } else { // move back elements backwards tail = dec(t, capacity); if (i &lt;= tail) { System.arraycopy(es, i + 1, es, i, back); } else { // Wrap around System.arraycopy(es, i + 1, es, i, capacity - (i + 1)); es[capacity - 1] = es[0]; System.arraycopy(es, 1, es, 0, t - 1); } es[tail] = null; return true; }}","link":"cn/ArrayDeque/"},{"title":"HashMap源码学习","text":"源码来自与OpenJDK 17 我选择个人认为重要的点进行总结 基本介绍 HashMap继承自AbstrachMap,实现Map，Clonable，Serializable接口 HashMap的最终储存变量为 1transient Node&lt;K,V&gt;[] table; HashMap最终的储存类型为Node&lt;K,V&gt; 实现了Map.Entry&lt;K,V&gt;接口 1static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt;{...} Node&lt;K,V&gt; 类中的equals调用Object类的equals函数进行比较 12345678public final boolean equals(Object o) { if (o == this) return true; return o instanceof Map.Entry&lt;?, ?&gt; e &amp;&amp; Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue()); } HashMap在进行hash计算时，调用传入对象的hashcode函数 1234static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} HashMap put函数 HashMap的put函数调用putval函数进行Entry&lt;K,V&gt;对象的插入 1234public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } putVal函数是HashMap类的精髓 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) //重点1 n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) //重点2 tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) //重点3 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash);//重点4 break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; //重点5 if (++size &gt; threshold) resize();// 重点6 afterNodeInsertion(evict); return null;} putVal函数先检查hashTable对象是否为null，或者table.length==0，若是则创建table。 在获得hash值后，使用table.length-1的二进制数（每个位都是1，如length==16 则 length-1二进制为1111），与hash值进行与运算，进行碰撞检测，使其合理的分配每一个table位。 table数组内的每一格会有两种可能的结构，若其中的Entry对象少于阈值，则使用链表储存，若Entry对象过多，则使用红黑树进行存储。 使用treeifyBin函数进行链表和红黑树的转换 在修改了结构后，需要对modCount进行+1，以实现快速失败功能 在HashMap 的大小超过一个阈值后，会使用resize()进行扩容 HashMap的扩容函数resize() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) { if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; //重点1 oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap);//重点2 else { // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab;} 在原容量大于MAXIMUM_CAPACITY时，新容量为Integer.MAX_VALUE，否则，新容量为老容量的两倍 若为红黑树结构，则调用TreeNode&lt;K,V&gt;.split()函数进行处理 链表结构，则进行拆分，让Entry对象的hash值与OldCap进行与运算（oldCap为2的幂 二进制只有一位是 1），这样只会有0和1两个结果。分别分配到对应的table数组内 HashMap get函数 HashMap的get函数调用getNode(key)函数对Key进行查找 1234public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(key)) == null ? null : e.value;} 12345678910111213141516171819final Node&lt;K,V&gt; getNode(Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n, hash; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; (hash = hash(key))]) != null) { //重点1 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //重点2 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); //重点3 } } return null;} 使用hash函数进行查找对应的table数组位置 若为红黑树，则调用getTreeNode()函数进行红黑树查找操作 若为链表则逐个查找 HashMap的remove函数调用removeNode函数进行删除 12345public V remove(Object key) { Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;} 12345678910111213141516171819202122232425262728293031323334353637383940final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) { //1 Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) //2 node = p; else if ((e = p.next) != null) { if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); //3 else { do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { node = e; break; } p = e; } while ((e = e.next) != null); //4 } } if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) { //5 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable);//6 else if (node == p) tab[index] = node.next; //7 else p.next = node.next; //8 ++modCount; //9 --size; afterNodeRemoval(node); return node; } } return null;} 此if括号内是寻找被删除Entry对象的寻找过程，寻找到后则删除 table[index]的第一个Entry对象 若此table[index]使用红黑树存储，则调用getTreeNode函数进行查找 若为链表结构，则依次查找 判断是否寻找到需要删除的Entry对象 调用removeTreeNode函数对红黑树中的Entry对象进行删除 Entry对象是table[index]中的第一个Entry 找到链表内需要删除的Entry对象 结构变化需要modCount++ HashMap中的红黑树 HashMap提供TreeNode&lt;K,V&gt;类进行红黑树的相关操作 TreeNode继承自LinkedHashMap.Entry&lt;K,V&gt;类 1static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt;{...} TreeNode类中的变量：除了红黑树所需的变量，还需要实现LinkedHashMap的前后节点 12345TreeNode&lt;K,V&gt; parent; // red-black tree linksTreeNode&lt;K,V&gt; left;TreeNode&lt;K,V&gt; right;TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletionboolean red; 当 HashMap遇到TreeNode节点会使用getTreeNode函数进行查找 h表示hash值 123final TreeNode&lt;K,V&gt; getTreeNode(int h, Object k) { return ((parent != null) ? root() : this).find(h, k, null);} 函数内部寻找到root节点后，调用find函数进行具体节点的查询 1234567891011121314151617181920212223242526final TreeNode&lt;K,V&gt; find(int h, Object k, Class&lt;?&gt; kc) { TreeNode&lt;K,V&gt; p = this; do { int ph, dir; K pk; TreeNode&lt;K,V&gt; pl = p.left, pr = p.right, q; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.find(h, k, kc)) != null) return q; else p = pl; } while (p != null); return null;} HashMap中链表到树的转变函数treeifyBin函数 1234567891011121314151617181920final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) { int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) { TreeNode&lt;K,V&gt; hd = null, tl = null; do { //重点1 TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); //重点2 }} 在进行红黑树的创建前，需要先将Node&lt;K,V&gt;节点，转化为红黑树的节点，并使用循环进行前后链表的连接 使用链表头传入treeify()函数，在treeify函数内进行红黑树的建立 treeify函数 1234567891011121314151617181920212223242526272829303132333435363738394041final void treeify(Node&lt;K,V&gt;[] tab) { TreeNode&lt;K,V&gt; root = null; for (TreeNode&lt;K,V&gt; x = this, next; x != null; x = next) { next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (root == null) { x.parent = null; x.red = false; root = x; } else { K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K,V&gt; p = root;;) { int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) { x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; root = balanceInsertion(root, x); break; } } } } moveRootToFront(tab, root);} Treeify()函数使用节点的hash值本身作为红黑树的比较依据。遇到hash值相等的情况时，使用tieBreakOrder()函数进行比较。在插入红黑树后，需要调用balanceInsertion()函数进行红黑树的平衡操作。最后使用moveRootToFront()函数将root节点写入tab。 TreeNode类可以使用putTreeVal()函数进行红黑树节点的插入，使用removeTreeNode函数进行树的删除。 TreeNode类提供一个split函数，对红黑树进行分割 12345678910111213141516171819202122232425262728293031323334353637383940414243444546final void split(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int index, int bit) { TreeNode&lt;K,V&gt; b = this; // Relink into lo and hi lists, preserving order TreeNode&lt;K,V&gt; loHead = null, loTail = null; TreeNode&lt;K,V&gt; hiHead = null, hiTail = null; int lc = 0, hc = 0; for (TreeNode&lt;K,V&gt; e = b, next; e != null; e = next) {//重点1 next = (TreeNode&lt;K,V&gt;)e.next; e.next = null; if ((e.hash &amp; bit) == 0) { //重点2 if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; } else { if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; } } if (loHead != null) { //重点3 if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else { tab[index] = loHead; if (hiHead != null) // (else is already treeified) loHead.treeify(tab); } } if (hiHead != null) { if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else { tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); } }} 对需要分割的红黑树，使用TreeNode中的链表结构逐一进行Entry对象的分配 对红黑树分割的依据是e.hash &amp; bit的值 对小于阈值的红黑树进行链表化，对超过阈值的红黑树化 在TreeNode中,需要用到左旋和右旋 相关函数rotateLeft和rotateRight 在TreeNode的增加和删除中需要平衡红黑树 相关函数balanceInsertion和balanceDeletion HashMap中的KeySet HashMap中可以通过KeySet单独提取出Key值 1234567891011121314151617181920212223242526272829303132333435final class KeySet extends AbstractSet&lt;K&gt; { public final int size() { return size; } public final void clear() { HashMap.this.clear(); } public final Iterator&lt;K&gt; iterator() { return new KeyIterator(); } public final boolean contains(Object o) { return containsKey(o); } public final boolean remove(Object key) { return removeNode(hash(key), key, null, false, true) != null; } public final Spliterator&lt;K&gt; spliterator() { return new KeySpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0); } public Object[] toArray() { return keysToArray(new Object[size]); } public &lt;T&gt; T[] toArray(T[] a) { return keysToArray(prepareArray(a)); } public final void forEach(Consumer&lt;? super K&gt; action) { Node&lt;K,V&gt;[] tab; if (action == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) { int mc = modCount; for (Node&lt;K,V&gt; e : tab) { for (; e != null; e = e.next) action.accept(e.key); } if (modCount != mc) throw new ConcurrentModificationException(); } }} 可使用其中的forEach函数进行KeySet的遍历 HashMap中的value类 HashMap中可以通过values类单独提取出value值 1234567891011121314151617181920212223242526272829303132final class Values extends AbstractCollection&lt;V&gt; { public final int size() { return size; } public final void clear() { HashMap.this.clear(); } public final Iterator&lt;V&gt; iterator() { return new ValueIterator(); } public final boolean contains(Object o) { return containsValue(o); } public final Spliterator&lt;V&gt; spliterator() { return new ValueSpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0); } public Object[] toArray() { return valuesToArray(new Object[size]); } public &lt;T&gt; T[] toArray(T[] a) { return valuesToArray(prepareArray(a)); } public final void forEach(Consumer&lt;? super V&gt; action) { Node&lt;K,V&gt;[] tab; if (action == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) { int mc = modCount; for (Node&lt;K,V&gt; e : tab) { for (; e != null; e = e.next) action.accept(e.value); } if (modCount != mc) throw new ConcurrentModificationException(); } }} 可以使用forEach函数进行遍历value HashMap中的EntrySet EntrySet提供HashMap的Set视图，对Set的操作都会反映在HashMap中。 123456789101112131415161718192021222324252627282930313233343536373839final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; { public final int size() { return size; } public final void clear() { HashMap.this.clear(); } public final Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() { return new EntryIterator(); } public final boolean contains(Object o) { if (!(o instanceof Map.Entry&lt;?, ?&gt; e)) return false; Object key = e.getKey(); Node&lt;K,V&gt; candidate = getNode(key); return candidate != null &amp;&amp; candidate.equals(e); } public final boolean remove(Object o) { if (o instanceof Map.Entry&lt;?, ?&gt; e) { Object key = e.getKey(); Object value = e.getValue(); return removeNode(hash(key), key, value, true, true) != null; } return false; } public final Spliterator&lt;Map.Entry&lt;K,V&gt;&gt; spliterator() { return new EntrySpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0); } public final void forEach(Consumer&lt;? super Map.Entry&lt;K,V&gt;&gt; action) { Node&lt;K,V&gt;[] tab; if (action == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) { int mc = modCount; for (Node&lt;K,V&gt; e : tab) { for (; e != null; e = e.next) action.accept(e); } if (modCount != mc) throw new ConcurrentModificationException(); } }}","link":"cn/HashMap/"},{"title":"ArrayList学习总结","text":"源码来自与OpenJDK 17 我选择个人认为重要的点进行总结 ArrayList继承自AbstractList,实现了List, RandomAccess, Cloneable, java.io.Serializable接口 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable ArrayList 使用数组进行元素的储存 1transient Object[] elementData; AbstractList 提供了一个modCount变量可以监控ArrayList对象的结构化修改（譬如add和remove） 1protected transient int modCount = 0; ArrayList中 add函数过程和调用的函数 add函数需要记录结构化修改次数 当底层数组容量满时需要调用grow()扩容 扩容数组大小一般使用原数组大小的1.5倍，当数组为空时，设置为默认大小(10)并申请空间，当内存空间过小的时候，申请最小的扩容空间，并复制原数组到新空间。 123456789101112131415161718192021222324252627public boolean add(E e) { modCount++; //结构化修改记录 add(e, elementData, size); return true;}private void add(E e, Object[] elementData, int s) { //真正调用的函数 if (s == elementData.length) elementData = grow(); //当底层数组容量不够时进行扩容 elementData[s] = e; size = s + 1;}//扩容函数private Object[] grow() { return grow(size + 1);}private Object[] grow(int minCapacity) { int oldCapacity = elementData.length; if (oldCapacity &gt; 0 || elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { int newCapacity = ArraysSupport.newLength(oldCapacity, minCapacity - oldCapacity, /* minimum growth */ oldCapacity &gt;&gt; 1 /* preferred growth */); return elementData = Arrays.copyOf(elementData, newCapacity); } else { return elementData = new Object[Math.max(DEFAULT_CAPACITY, minCapacity)]; }} ArrayList使用add(int index, E element)在指定位置加入元素时，需要将index和index后的元素向后移动一位(使用copy)，再插入元素。 1234567891011public void add(int index, E element) { rangeCheckForAdd(index); modCount++; final int s; Object[] elementData; if ((s = size) == (elementData = this.elementData).length) elementData = grow(); System.arraycopy(elementData, index,elementData, index + 1,s - index); elementData[index] = element; size = s + 1; } ArrayList的clone函数返回一个modCount=0的对象 1234567891011public Object clone() { try { ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; } catch (CloneNotSupportedException e) { // this shouldn't happen, since we are Cloneable throw new InternalError(e); }} ArrayList可以返回一个拥有全部List内元素的源类对象 123456789101112public Object[] toArray() { return Arrays.copyOf(elementData, size); } public &lt;T&gt; T[] toArray(T[] a) { if (a.length &lt; size) // Make a new array of a's runtime type, but my contents: return (T[]) Arrays.copyOf(elementData, size, a.getClass()); System.arraycopy(elementData, 0, a, 0, size); if (a.length &gt; size) a[size] = null; return a; } ArrayList的get(int index)是经过检查的 1234public E get(int index) { Objects.checkIndex(index, size);//检查index是否合法 return elementData(index); } ArrayList的remove(index)函数也是经过检查的 remove的原理是将index后的元素都向前移动一位(使用copy)。将最后一位设置为null，当然这也需要modCount++ 1234567891011121314151617public E remove(int index) { Objects.checkIndex(index, size); final Object[] es = elementData; @SuppressWarnings(&quot;unchecked&quot;) E oldValue = (E) es[index]; fastRemove(es, index); return oldValue; }private void fastRemove(Object[] es, int i) { modCount++; final int newSize; if ((newSize = size - 1) &gt; i) System.arraycopy(es, i + 1, es, i, newSize - i); es[size = newSize] = null; } 使用clear()函数，会将所有元素设置为null，List大小不变，modCount++。 123456public void clear() { modCount++; final Object[] es = elementData; for (int to = size, i = size = 0; i &lt; to; i++) es[i] = null; } retainAll(Collection&lt;?&gt; c) 保留所有c中含有的元素 由于contains(Object o）无法判断时间复杂度 若contain(Object o) O(1)则时间复杂度O(n) 123456789101112131415161718192021222324252627282930313233public boolean retainAll(Collection&lt;?&gt; c) { return batchRemove(c, true, 0, size); } boolean batchRemove(Collection&lt;?&gt; c, boolean complement, final int from, final int end) { Objects.requireNonNull(c); final Object[] es = elementData; int r; // Optimize for initial run of survivors for (r = from;; r++) { if (r == end) return false; if (c.contains(es[r]) != complement) break; } int w = r++; try { for (Object e; r &lt; end; r++) if (c.contains(e = es[r]) == complement) es[w++] = e; } catch (Throwable ex) { // Preserve behavioral compatibility with AbstractCollection, // even if c.contains() throws. System.arraycopy(es, r, es, w, end - r); w += end - r; throw ex; } finally { modCount += end - w; shiftTailOverGap(es, w, end); } return true; } ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。 12345678public void trimToSize() { modCount++; if (size &lt; elementData.length) { elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); } }","link":"cn/ArrayList/"},{"title":"TreeMap学习总结","text":"源码来自与OpenJDK 17 我选择个人认为重要的点进行总结 TreeMap继承自AbstractMap&lt;K,V&gt; 实现NavigableMap&lt;K,V&gt;, Cloneable , java.io.Serializable接口 Treemap中存在 modCount用来指示TreeMap对象结构化修改的次数，但是由于modCount被private形容，所以不可获取此变量。类中时使用modCount用于实现快速失败机制。 1private transient int modCount = 0; TreeMap可以传入Map对象进行红黑树的建立 若传入的Map对象是实现了SortedMap接口的类的实例，则使用buildFromSorted函数进行树的建立，以节省运行时间。最终使用buildFromSorted函数递归调用进行构建。 若不是，则调用put函数逐个插入红黑树节点 1234567891011121314151617181920212223242526272829 public TreeMap(Map&lt;? extends K, ? extends V&gt; m) { comparator = null; putAll(m); }public void putAll(Map&lt;? extends K, ? extends V&gt; map) { int mapSize = map.size(); if (size==0 &amp;&amp; mapSize!=0 &amp;&amp; map instanceof SortedMap) { if (Objects.equals(comparator, ((SortedMap&lt;?,?&gt;)map).comparator())) { ++modCount; try { buildFromSorted(mapSize, map.entrySet().iterator(), null, null); } catch (java.io.IOException | ClassNotFoundException cannotHappen) { } return; } } super.putAll(map); }private void buildFromSorted(int size, Iterator&lt;?&gt; it, java.io.ObjectInputStream str, V defaultVal) throws java.io.IOException, ClassNotFoundException { this.size = size; root = buildFromSorted(0, 0, size-1, computeRedLevel(size), it, str, defaultVal); } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354 private final Entry&lt;K,V&gt; buildFromSorted(int level, int lo, int hi, int redLevel, Iterator&lt;?&gt; it, java.io.ObjectInputStream str, V defaultVal) throws java.io.IOException, ClassNotFoundException { if (hi &lt; lo) return null; int mid = (lo + hi) &gt;&gt;&gt; 1; Entry&lt;K,V&gt; left = null; if (lo &lt; mid) left = buildFromSorted(level+1, lo, mid - 1, redLevel, it, str, defaultVal); // extract key and/or value from iterator or stream K key; V value; if (it != null) { if (defaultVal==null) { Map.Entry&lt;?,?&gt; entry = (Map.Entry&lt;?,?&gt;)it.next(); key = (K)entry.getKey(); value = (V)entry.getValue(); } else { key = (K)it.next(); value = defaultVal; } } else { // use stream key = (K) str.readObject(); value = (defaultVal != null ? defaultVal : (V) str.readObject()); } Entry&lt;K,V&gt; middle = new Entry&lt;&gt;(key, value, null); // color nodes in non-full bottommost level red if (level == redLevel) middle.color = RED; if (left != null) { middle.left = left; left.parent = middle; } if (mid &lt; hi) { Entry&lt;K,V&gt; right = buildFromSorted(level+1, mid+1, hi, redLevel, it, str, defaultVal); middle.right = right; right.parent = middle; } return middle;} 1234public void putAll(Map&lt;? extends K, ? extends V&gt; m) { for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) put(e.getKey(), e.getValue());} TreeMap的put函数调用内部的private put函数进行插入。 若树为空，则调用addEntryToEmptyMap初始化树 若不为空，通过比较进行树的查找。若寻找到对应节点，则修改value值。若未查找到值，则使用addEntry函数进行红黑树节点的插入 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 public V put(K key, V value) { return put(key, value, true); }private V put(K key, V value, boolean replaceOld) { Entry&lt;K,V&gt; t = root; if (t == null) { addEntryToEmptyMap(key, value); return null; } int cmp; Entry&lt;K,V&gt; parent; // split comparator and comparable paths Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) { do { parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else { V oldValue = t.value; if (replaceOld || oldValue == null) { t.value = value; } return oldValue; } } while (t != null); } else { Objects.requireNonNull(key); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do { parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else { V oldValue = t.value; if (replaceOld || oldValue == null) { t.value = value; } return oldValue; } } while (t != null); } addEntry(key, value, parent, cmp &lt; 0); return null; } addEntry函数是红黑树的插入函数，在创建了结点之后，需要进行红黑树性质的恢复，使用fixAfterinsertion函数，最后修改size和modCount 12345678910private void addEntry(K key, V value, Entry&lt;K, V&gt; parent, boolean addToLeft) { Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); if (addToLeft) parent.left = e; else parent.right = e; fixAfterInsertion(e); size++; modCount++;} 12345678910111213141516171819202122232425262728293031323334353637383940private void fixAfterInsertion(Entry&lt;K,V&gt; x) { x.color = RED; while (x != null &amp;&amp; x != root &amp;&amp; x.parent.color == RED) { if (parentOf(x) == leftOf(parentOf(parentOf(x)))) { Entry&lt;K,V&gt; y = rightOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { setColor(parentOf(x), BLACK); setColor(y, BLACK); setColor(parentOf(parentOf(x)), RED); x = parentOf(parentOf(x)); } else { if (x == rightOf(parentOf(x))) { x = parentOf(x); rotateLeft(x); } setColor(parentOf(x), BLACK); setColor(parentOf(parentOf(x)), RED); rotateRight(parentOf(parentOf(x))); } } else { Entry&lt;K,V&gt; y = leftOf(parentOf(parentOf(x))); if (colorOf(y) == RED) { setColor(parentOf(x), BLACK); setColor(y, BLACK); setColor(parentOf(parentOf(x)), RED); x = parentOf(parentOf(x)); } else { if (x == leftOf(parentOf(x))) { x = parentOf(x); rotateRight(x); } setColor(parentOf(x), BLACK); setColor(parentOf(parentOf(x)), RED); rotateLeft(parentOf(parentOf(x))); } } } root.color = BLACK;} TreeMap使用get函数获取元素。函数内部最终使用getEntry进行红黑树的查找 123456789101112131415161718192021222324 public V get(Object key) { Entry&lt;K,V&gt; p = getEntry(key); return (p==null ? null : p.value); }final Entry&lt;K,V&gt; getEntry(Object key) { // Offload comparator-based version for sake of performance if (comparator != null) return getEntryUsingComparator(key); Objects.requireNonNull(key); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K,V&gt; p = root; while (p != null) { int cmp = k.compareTo(p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; } return null; } TreeMap使用remove函数删除指定的key所在的Entry对象，最终调用deleteEntry进行删除 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public V remove(Object key) { Entry&lt;K,V&gt; p = getEntry(key); if (p == null) return null; V oldValue = p.value; deleteEntry(p); return oldValue;}private void deleteEntry(Entry&lt;K,V&gt; p) { modCount++; size--; // If strictly internal, copy successor's element to p and then make p // point to successor. if (p.left != null &amp;&amp; p.right != null) { Entry&lt;K,V&gt; s = successor(p); p.key = s.key; p.value = s.value; p = s; } // p has 2 children // Start fixup at replacement node, if it exists. Entry&lt;K,V&gt; replacement = (p.left != null ? p.left : p.right); if (replacement != null) { // Link replacement to parent replacement.parent = p.parent; if (p.parent == null) root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else p.parent.right = replacement; // Null out links so they are OK to use by fixAfterDeletion. p.left = p.right = p.parent = null; // Fix replacement if (p.color == BLACK) fixAfterDeletion(replacement); } else if (p.parent == null) { // return if we are the only node. root = null; } else { // No children. Use self as phantom replacement and unlink. if (p.color == BLACK) fixAfterDeletion(p); if (p.parent != null) { if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; } }}","link":"cn/TreeMap/"},{"title":"Hive 开发小芝士","text":"在进行关于Hive的开发的时候遇到了一个问题，简直活久见，简单来说 select count(1) 返回 0 而Hive数据库内存在50条数据。 活久见 在进行关于Hive的开发的时候遇到了一个问题，简直活久见，简单来说 select count(1) 返回 0 而Hive数据库内存在50条数据。 我需要使用jdbc执行一项命令: 1select count(1) from aaa.parttest where day = 20220318; 他的返回值为 0 但是问题是，这个表中，是存在数据的 1select * from aaa.parttest where day = 20220318; 返回数据： 这里面是有数据的，嗯？？？ 和公司里导师说，导师：真的是活久见。 然后，然后他就去查百度了。 可能这就是为什么他是Leader候选人的原因吧。求知的心态很重要的呀，而我感觉这是一个问题。在百度的同时，也建议我去写一篇文章，来记录这一次的问题。然而我竟然都没有想到。哎！这算是个什么问题呢，就算是积极性的问题吧，想到实习一个月后当时那句“从学生到打工人身份的转变”，说的真在理。 产生这个现象的原因 所以到底是为什么呢？ 在网上的基本回答都是这样的： 这是hive中的一个优化参数导致的，对于一些使用频率可能很高的sql会进行查询优化，会将这个参数[hive.compute.query.using.stats]设置为true(默认是false)，这样的话，Hive在执行某些查询时，例如select count(1)，只利用元数据存储中保存的状态信息返回结果，从而提高了响应速度。 可以使用set hive.compute.query.using.stats=false;进行优化参数的修改 在查询hive后发现，确实～～～～ 1set hive.compute.query.using.stats; 但是，问题是我只在这一个表中使用count(1) 命令才会出现，让我误以为这是一种什么bug（但其实不是啦）。 这个表为什么特殊的地方呢？是什么出发了这样的现象呢？ 原因在于我对表进行创建时，分区的创建来自于下面这条命令 1alter table aaa.parttest add partition(day = 20220318); 写入数据来自于治理系统的数据集成功能，也就是使用datax进行数据集成。在分区路径下建立文件夹，写入数据文件part_bac19ea3_d70c_47d6_8b34_076fe97161ad.gz。 但是，这个操作好像并没有对表的元信息进行统计更新，但执行的时候又不走MR，那就是只有返回0了。 在看过这篇文章后,执行了load命令，对数据进行加载 1load data inpath &quot;hdfs://getui-bi-test-new/apps/hive/warehouse/aaa.db/parttest/day=20220318/part_bac19ea3_d70c_47d6_8b34_076fe97161ad.gz&quot; into table parttest partition(day = 20220318); 以及 123explain select count(1) from aaa.parttest where day = 20220318;explain select count(1) from parttest where day = 20220399;describe formatted parttest; 发现select count(1) from aaa.parttest where day = 20220318命令经过了MapReduce操作。 可以发现经过load命令后，并没有对表进行元数据统计，但是使用了MR进行数据统计，未经过load的分区依旧没有MR，返回值 0 。 查阅Hive手册可知，对于新创建的表和/或分区(通过INSERT OVERWRITE命令填充)，默认情况下会自动计算统计信息。但是经过了load命令后insert一行新数据 ，好像并没有进行更新。 那么，既然没有统计元数据，手动统计一下会如何呢？ 1analyze table parttest compute statistics no scans; //当指定了可选参数 NOSCAN 时，该命令将不会扫描文件，因此应该是快速的。代替所有统计信息，它仅收集：文件数 、物理大小(以字节为单位)。 这条命令跑了一分钟… 12select count(*) from parttest where day = 20220318;explain select count(*) from parttest where day = 20220318; 结果是 51 正确!，并且命令没有使用MR，使用了表元数据。 ![](https://mockingjay-1257222092.cos.ap-guangzhou.myqcloud.com/截屏2022-03-20 00.33.30.png) 同时 ，也可以使用 1select count(*) from parttest where day = 20220318 limit 1; 进行查询 1explain select count(*) from parttest where day = 20220318 limit 1; 其中可见 select count(*)命令经过了MR。 总结 在进行使用 select count(1)统计数据量时，可能会出现数据量和现实不符合的情况，可能是由于select count(1)命令使用了表的元数据或统计信息，同时元数据统计不及时导致结果和现实不同。可以先手动进行统计，再使用select count(1)命令。或者 直接使用select count(1) ... limit 1可以进行MR的操作。 参考文章 [1]https://www.jianshu.com/p/6d4485262e0a [2]https://www.cnblogs.com/coco2015/p/15870532.html [3]https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/LanguageManual_DML.html","link":"cn/hive%E5%BC%80%E5%8F%91%E5%B0%8F%E8%8A%9D%E5%A3%AB/"},{"title":"使用hugo生成静态博客并部署在GitHub上","text":"简介 hugo是一个用Go语言编写的静态网页生成器，只需要一个命令 hugo 就可以在几秒钟内生成一个静态的博客页面，被称为世界上最快的网站构建框架，使hugo称为最受欢迎且最热门的静态网站生成器之一。 我之前使用的是wordpress作为博客的载体，但是wordpress太笨重，对markdown格式的支持不是很好，且打开速度极慢，所以就使用hugo进行博客的撰写，生成静态博客后就可以部署在云服务器上或者GitHub上，这样在国内搜到的云服务器上的博客http://www.mockingj.cn，在国外就是github上的pageshttps://caecarxu.github.io/,这样国内外都有一个比较好的访问速度。 1、安装hugo 我使用的系统是windows10，在hugo的官方文档中，使用[Scoop](scoop install hugo-extended)作为包管理器，用一个命令行就完成下载 1scoop install hugo 或者是安装hugo的extend版本 1scoop install hugo-extended 安装成功后验证一下，在命令行中输入 1hugo version 若输出 1hugo v0.86.1-F6821B88 windows/amd64 BuildDate=2021-07-30T10:13:35Z VendorInfo=gohugoio 则为成功安装 当然不想用windows的包管理器，可以使用git进行下载 1git clone https://github.com/gohugoio/hugo.git #将hugo下载至命令行所在文件夹 然后在右击windows中的此电脑，在选择框中选择属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量-&gt;Path-&gt;编辑，将刚刚的hugo文件夹加入Path中 重启电脑，执行hugo version命令，验证安装。 2、使用hugo 在命令行中执行hugo help可以查看hugo的基本使用方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172hugo is the main command, used to build your Hugo site.Hugo is a Fast and Flexible Static Site Generatorbuilt with love by spf13 and friends in Go.Complete documentation is available at http://gohugo.io/.Usage: hugo [flags] hugo [command]Available Commands: config Print the site configuration convert Convert your content to different formats deploy Deploy your site to a Cloud provider. env Print Hugo version and environment info gen A collection of several useful generators. help Help about any command import Import your site from others. list Listing out various types of content mod Various Hugo Modules helpers. new Create new content for your site server A high performance webserver version Print the version number of HugoFlags: -b, --baseURL string hostname (and path) to the root, e.g. http://spf13.com/ -D, --buildDrafts include content marked as draft -E, --buildExpired include expired content -F, --buildFuture include content with publishdate in the future --cacheDir string filesystem path to cache directory. Defaults: $TMPDIR/hugo_cache/ --cleanDestinationDir remove files from destination not found in static directories --config string config file (default is path/config.yaml|json|toml) --configDir string config dir (default &quot;config&quot;) -c, --contentDir string filesystem path to content directory --debug debug output -d, --destination string filesystem path to write files to --disableKinds strings disable different kind of pages (home, RSS etc.) --enableGitInfo add Git revision, date and author info to the pages -e, --environment string build environment --forceSyncStatic copy all files when static is changed. --gc enable to run some cleanup tasks (remove unused cache files) after the build -h, --help help for hugo --i18n-warnings print missing translations --ignoreCache ignores the cache directory --ignoreVendor ignores any _vendor directory --ignoreVendorPaths string ignores any _vendor for module paths matching the given Glob pattern -l, --layoutDir string filesystem path to layout directory --log enable Logging --logFile string log File path (if set, logging enabled automatically) --minify minify any supported output format (HTML, XML etc.) --noChmod don't sync permission mode of files --noTimes don't sync modification time of files --path-warnings print warnings on duplicate target paths etc. --poll string set this to a poll interval, e.g --poll 700ms, to use a poll based approach to watch for file system changes --print-mem print memory usage to screen at intervals --quiet build in quiet mode --renderToMemory render to memory (only useful for benchmark testing) -s, --source string filesystem path to read files relative from --templateMetrics display metrics about template executions --templateMetricsHints calculate some improvement hints when combined with --templateMetrics -t, --theme strings themes to use (located in /themes/THEMENAME/) --themesDir string filesystem path to themes directory --trace file write trace to file (not useful in general) -v, --verbose verbose output --verboseLog verbose logging -w, --watch watch filesystem for changes and recreate as neededAdditional help topics: hugo check Contains some verification checksUse &quot;hugo [command] --help&quot; for more information about a command. 这里我们只需要使用几个命令就可以了: 建立博客，名为test 1hugo new site test 输出 123456789101112Congratulations! Your new Hugo site is created in C:\\git\\hugo_blog\\test.Just a few more steps and you're ready to go:1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the &quot;hugo new theme &lt;THEMENAME&gt;&quot; command.2. Perhaps you want to add some content. You can add single files with &quot;hugo new &lt;SECTIONNAME&gt;\\&lt;FILENAME&gt;.&lt;FORMAT&gt;&quot;.3. Start the built-in live server via &quot;hugo server&quot;.Visit https://gohugo.io/ for quickstart guide and full documentation. 这样就生成了一个博客，相关的文件都在test文件夹内 进入博客文件夹内 1cd test test文件夹内会有一个config.toml的文件,这个文件是hugo的配置文件 1baseURL = &quot;http://example.org/&quot; languageCode = &quot;en-us&quot;title = &quot;My New Hugo Site&quot; baseURL是博客的网址，生成的博客资源都是基于这个网址进行获取的 title是网站的标题，就像这个 现在可以使用以下命令运行以下hugo博客的预览 1hugo server 输出 12345678910111213141516171819202122232425Start building sites …hugo v0.86.1-F6821B88 windows/amd64 BuildDate=2021-07-30T10:13:35Z VendorInfo=gohugoioWARN 2021/08/08 11:08:08 found no layout file for &quot;HTML&quot; for kind &quot;taxonomy&quot;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination.WARN 2021/08/08 11:08:08 found no layout file for &quot;HTML&quot; for kind &quot;taxonomy&quot;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination.WARN 2021/08/08 11:08:08 found no layout file for &quot;HTML&quot; for kind &quot;home&quot;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. | EN-------------------+----- Pages | 3 Paginator pages | 0 Non-page files | 0 Static files | 0 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0Built in 37 msWatching for changes in C:\\git\\hugo_blog\\test\\{archetypes,content,data,layouts,static}Watching for config changes in C:\\git\\hugo_blog\\test\\config.tomlEnvironment: &quot;development&quot;Serving pages from memoryRunning in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRenderWeb Server is available at http://localhost:1313/ (bind address 127.0.0.1)Press Ctrl+C to stop 倒数第二行显示了博客的地址localhost:1313,若1313地址被使用，则会随机使用一个端口 但是现在的博客还什么都没有，打开就是一片空白 所以现在需要加入主题设计 3、加入主题并运行博客 在网上查询之后，发现HUGO官方搭建的theme主题站,大量的开源主题可供选择。 基本上所有的主题都自带安装的方法 这里我选择了一个主题进行调试https://themes.gohugo.io/themes/blist-hugo-theme/ 在网页下方的说明内，找到主题安装的方法 或者，直接去主题的github网址，里面的readme.md中有说明主题的下载和使用方式，两个方法都是一样的 于是，根据主题作者提供的命令/ 1git clone https://github.com/apvarun/blist-hugo-theme.git themes/blist ##获得主题 下载完成后，将 package.json 文件从 themes/showcase 文件夹复制到你的 Hugo 网站根文件夹，然后运行 npm install 进入themes/blist/exampleSite文件夹中，复制exampleSite中的文件，替换掉Hugo网站根文件夹中的相同命名的文件 执行命令 1hugo serve 这个时候有可能会因为一些twitter或者facebook的api无法访问而导致的问题 1234567Start building sites …hugo v0.86.1-F6821B88 windows/amd64 BuildDate=2021-07-30T10:13:35Z VendorInfo=gohugoioERROR 2021/08/08 15:05:14 Failed to get JSON resource &quot;https://api.twitter.com/1/statuses/oembed.json?id=1085870671291310081&amp;omit_script=true&quot;: Get &quot;https://api.twitter.com/1/statuses/oembed.json?id=1085870671291310081&amp;omit_script=true&quot;: dial tcp 162.125.2.6:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.If you feel that this should not be logged as an ERROR, you can ignore it by adding this to your site config:ignoreErrors = [&quot;error-remote-getjson&quot;]Error: Error building site: &quot;C:\\git\\hugo_blog\\test\\content\\blog\\rich-content.md:1:1&quot;: timed out initializing value. You may have a circular loop in a shortcode, or your site may have resources that take longer to build than the `timeout` limit in your Hugo config file.Built in 37375 ms 这个时候就去寻找报错中使用这些资源的文件 直接去掉就好了，反正国内也用不到这些 在执行一次命令 1hugo serve 成功运行博客！！！ 4、将博客部署到github上 这部分需要使用到一个叫做github page的东西，是GitHub提供的一个网页寄存服务，可以寄存静态页面，这个特性就特别适合博客的搭建。 首先再Github上创建一个 Repository，命名为Github名字.github.io，譬如我的仓库caecarxu.github.io，这样就可以生成一个用户页面 在hugo server调式页面完成后，使用命令hugo生成hugo静态页面。 注意！！！ 在生成静态页面之前要把config.toml文件中的baseURL修改为自己博客的网址，譬如 1baseURL = &quot;https://caecarxu.github.io/&quot; 1hugo ##生成静态页面文件 在命令执行后，出现一个public文件夹，里面就是网站的静态页面文件 进入public文件夹，使用git上传文件 123456cd publicgit init ##初始化仓库git remote add origin https://github.com/caecarxu/caecarxu.github.io.git ##链接远程仓库git add .git commit -m &quot;first commit&quot;git push -u origin master 在此之后更新文章，使用hugo生成新的静态页面，并使用git push进行同步 12345cd publicgit add .git statusgit commit -m &quot;add blog post&quot;git push 提示以下，commit -m 后面的东西是此次提交的备注，通常用来说明提交人的名字","link":"cn/hugo%E7%94%9F%E6%88%90%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"title":"Java内存模型（JMM）","text":"Java内存模型基础 在并发编程中需要解决的两个问题 线程之间的通信 在共享内存的并发模型里，线程之间共享程序的公共状态，通过读写内存中的公共状态进行饮食通信。 在消息传递的并发模型里，线程之间没有公共状态，线程必须通过发送消息来进行通信。 线程之间的同步 在共享内存并发模型里，同步是显式进行的，程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 指令的重排序问题 在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。 重排序分3种类型 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 JSR-133的happens-before规则 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作（指令前后不改变顺序）。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁（先加锁后解锁）。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读（对volatile域先写后读）。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 happens规则的用处是提供了一个通用的内存重排序规则，避免程序员为了理解JMM提供的内存可见性取学习复杂的重排序规则 重排序 数据依赖性 如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。 上面3种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变。编译器和处理器不会进行重排序。 as-if-serial语义 as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序 程序顺序规则 即 1）A happens-before B。 2）B happens-before C。 3）A happens-before C。 这里A happens-before B，但实际执行时B却可以排在A之前执行。如果A happens-before B，JMM并不要求A一定要在B之前执行。JMM仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作A的执行结果不需要对操作B可见；而且重排序操作A和操作B后的执行结果，与操作A和操作B按happens-before顺序执行的结果一致。在这种情况下，JMM会认为这种重排序并不非法（not illegal），JMM允许这种重排序。 顺序一致性 顺序一致性内存模型是一个理论参考模型，在设计的时候，处理器的内存模型和编程语言的内存模型都会以顺序一致性内存模型作为参照。 数据竞争与顺序一致性 数据竞争的定义 在一个线程中写一个变量， 在另一个线程读同一个变量， 而且写和读没有通过同步来排序。 解决数据竞争的内存模型 顺序一致性内存模型 特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。 未同步程序的执行特性 对于未同步或未正确同步的多线程程序，JMM只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，Null，False） 未同步程序在顺序一致性模型和JMM内存模型中的执行特性有如下几个差异。 序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按多线程程序的顺序执行 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序。 JMM不保证对64位的long型和double型变量的写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性 volatile内存语义 volatile的特性 可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性。 volatile写-读的内存语义 volatile写的内存语义如下 当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。 volatile读的内存语义如下。 当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 volatile内存语义的实现 volatile内存语义的放重排序操作 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 插入内存屏障 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 锁的内存语义 锁的释放-获取建立的happens-before关系 加锁后的代码在执行中，遵循happens-before规则 锁的释放和获取的内存语义 线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。 线程B获取一个锁，实质上是线程B接收了之前某个线程发出的（在释放这个锁之前对共享变量所做修改的）消息。 线程A释放锁，随后线程B获取这个锁，这个过程实质上是线程A通过主内存向线程B发送消息。 锁内存语义的实现 在ReentrantLock中，调用lock()方法获取锁；调用unlock()方法释放锁。 ReentrantLock的实现依赖于Java同步器框架AbstractQueuedSynchronizer（本文简称之为AQS）。AQS使用一个整型的volatile变量（命名为state）来维护同步状态，马上我们会看到，这个volatile变量是ReentrantLock内存语义实现的关键。 final域的内存语义 final域的重排序规则 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 写final域的重排序规则 写final域的重排序规则禁止把final域的写重排序到构造函数之外。这个规则的实现包含下面2个方面。 JMM禁止编译器把final域的写重排序到构造函数之外。 编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。这个屏障禁止处理器把final域的写重排序到构造函数之外。 读final域的重排序规则 读final域的重排序规则是，在一个线程中，初次读对象引用与初次读该对象包含的final域，JMM禁止处理器重排序这两个操作。编译器会在读final域操作的前面插入一个LoadLoad屏障。 这可以确保在读一个对象的final域之前，一定会先读包含这个final域的对象的引用。 final域为引用类型 写final域的重排序规则对编译器和处理器增加了如下约束 在构造函数内对一个final引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 在构造函数返回前，被构造对象的引用不能为其他线程所见，因为此时的final域可能还没有被初始化。在构造函数返回后，任意线程都将保证能看到final域正确初始化之后的值。","link":"cn/java%E5%B9%B6%E5%8F%91(2)/"},{"title":"java并发编程的底层实现原理","text":"java并发编程的底层实现原理 volatile的定义与实现原理 定义：Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量 volatile在指令级（Lock指令/内存嗅探）上做的事情： 将当前处理器缓存行的数据写回到系统内存 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效 volatile使用内存屏障实现禁止重排序。 synchronized的实现原理和应用 synchronized实现同步的基础：Java中的每一个对象都可以作为锁： 对于同步方法，锁是当前示例对象 对于静态同步方法，锁是当前类的Class对象 对于同步方法块，锁是Sychonized括号里配置的对象 Synchonidze在JVM里的实现原理： JVM基于进入和退出Monitor对象来实现方法同步和代码块同步。 代码块同步是使用monitorenter和monitorexit指令实现的，而方法同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明 Java对象头 Java对象头的组成结构 其中MarkWord里的储存内容及其状态变化 锁的升级于对比 偏向锁 ​ 一般来说，锁总是由同一线程多次获得，为了减少CAS操作，引入偏向锁。当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID。在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。 偏向锁的撤销 当其他线程尝试竞争偏向锁时，偏向锁才会撤销 偏向锁撤销时需要等待全局安全点 当存在线程竞争时，检查持有偏向锁的线程是否或者，若否，则设置为无锁，若是，则升级为轻量级锁 关闭偏向锁 通过JVM参数关闭偏向锁：-XX:-UseBiasedLocking=false 轻量级锁 轻量级锁加锁 在线程执行同步块之前，JVM会现在当前线程的栈帧中创建用于储存锁记录的空间，并将对象头中的Mark Word复制到锁记录中。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 轻量级锁解锁 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。结束竞争失败线程的自旋，并阻塞。 重量级锁 在重量级锁状态下，除了获得锁的线程会运行之外，其他的线程都是阻塞的 java Synchonized原理图 原子操作的实现原理 原子操作的相关术语 CAS 比较并交换（Compare and Swap） CPU流水线 （CPU pipeline） 内存顺序冲突（Memory order violation） 处理器实现原子操作 使用总线锁：使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占共享内存。 使用缓存锁：存区域如果被缓存在处理器的缓存行中，并且在Lock操作期间被锁定，那么当它执行锁操作回写到内存时，处理器不在总线上声言LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性 Java实现原子操作的方法 使用循环CAS实现 CAS的三大问题 ABA问题 循环时间长开销大 只能保证一个共享变量的原子操作 使用锁机制实现原子操作 ​ JVM内部实现了很多种锁机制，有偏向锁、轻量级锁和互斥锁。有意思的是除了偏向锁，JVM实现锁的方式都用了循环CAS，即当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁","link":"cn/java%E5%B9%B6%E5%8F%91(1)/"},{"title":"Java并发编程基础","text":"java并发编程基础 线程简介 什么是线程、线程的特点、与进程的区别 线程定义：现代操作系统调度的最小单元 特点： 可以共享同一个进程内部的内存和资源 允许一个应用程序在同一地址空间内有多个不同活动线程 线程之间也是独立的，一个线程不知道父进程中是否有其他线程 与进程的区别： 进程是资源分配的最小单位，线程是CPU调度的最小单位 线程在进程下进行 进程之间不会相互影响，同一个进程下的线程之间会相互影响 不同进程间数据和资源很难共享，进程与进程之间相互隔离 使用线程的原因 更多的处理器核心 更快的相应时间 更好的编程模型 线程状态 线程状态图 线程状态转化图 启动和终止线程 线程的创建 通过实现 Runnable 接口，实现run接口； 通过继承 Thread 类本身，重写run函数； 创建 Callable 接口的实现类，并实现 call() 方法，该 call() 方法将作为线程执行体，并且有返回值 创建 Callable 实现类的实例，使用 FutureTask 类来包装 Callable 对象。 线程中断 中断可以理解为线程的一个标识位属性，其他线程通过调用该线程的interrupt()方法对其进行中断操作。 线程通过方法isInterrupted()来进行判断是否被中断，调用静态方法Thread.interrupted()对当前线程的中断标识位进行复位 安全地终止线程 使用中断进行线程中止 使用boolean变量控制中断 线程间通信 使用volatile和synchronized关键字 关键字volatile可以用来修饰字段（成员变量），就是告知程序任何对该变量的访问均需要从共享内存中获取，而对它的改变必须同步刷新回共享内存，它能保证所有线程对变量访问的可见性。 关键字synchronized可以修饰方法或者以同步块的形式来进行使用，它主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中，它保证了线程对变量访问的可见性和排他性。 synchronized对同步块使用monitorenter和monitorexit指令进行同步，其本质是对一个**对象监视器（monitor）**进行获取 访问对象需要获得Object的监视器，若获取失败则进入同步队列，线程状态变为BLOCKED。当访问Object的前驱（获得了锁的线程）释放了锁，则该释放操作唤醒阻塞在同步队列中的线程，使其重新尝试对监视器的获取。 等待/通知机制 调用wait()、notify()以及notifyAll()时需要注意的细节，如下： 使用wait()、notify()和notifyAll()时需要先对调用对象加锁。 调用wait()方法后，线程状态由RUNNING变为WAITING，并将当前线程放置到对象的等待队列。 notify()或notifyAll()方法调用后，等待线程依旧不会从wait()返回，需要调用notify()或notifAll()的线程释放锁之后，等待线程才有机会从wait()返回。 notify()方法将等待队列中的一个等待线程从等待队列中移到同步队列中，而notifyAll()方法则是将等待队列中所有的线程全部移到同步队列，被移动的线程状态由WAITING变为BLOCKED。 从wait()方法返回的前提是获得了调用对象的锁。 通知/等待基本过程 等待/通知的经典范式 等待方遵循原则 获取对象的锁。 如果条件不满足，那么调用对象的wait()方法，被通知后仍要检查条件。 条件满足则执行对应的逻辑。 123456synchronized(对象) { while(条件不满足) { 对象.wait(); } 对应的处理逻辑} 通知方遵循原则 获得对象的锁。 改变条件。 通知所有等待在对象上的线程。 1234synchronized(对象) { 改变条件 对象.notifyAll();} Thread.join()的使用 如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待thread线程终止之后才从thread.join()返回。线程Thread除了提供join()方法之外，还提供了join(long millis)和join(long millis,int nanos)两个具备超时特性的方法。这两个超时方法表示，如果线程thread在给定的超时时间里没有终止，那么将会从该超时方法中返回。 ThreadLocal的使用 ThreadLocal，即线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构。这个结构被附带在线程上，也就是说一个线程可以根据一个ThreadLocal对象查询到绑定在这个线程上的一个值。可以通过set(T)方法来设置一个值，在当前线程下再通过get()方法获取到原先设置的值。 线程池初探 过多的线程会造成操作系统频繁的进行线程上下文切换，无故增加系统的负载。 线程池技术预先创建了若干数量的线程，并且不能由用户直接对线程的创建进行控制，在这个前提下重复使用固定或较为固定数目的线程来完成任务的执行。一方面，消除了频繁创建和消亡线程的系统资源开销，另一方面，面对过量任务的提交能够平缓的劣化。 线程池的本质就是使用了一个线程安全的工作队列连接工作者线程和客户端线程，客户端线程将任务放入工作队列后便返回，而工作者线程则不断地从工作队列上取出工作并执行。","link":"cn/java%E5%B9%B6%E5%8F%91(3)/"},{"title":"Java中的AQS","text":"前言 java中的有内置锁，关键字为synchronized 当synchronized作用于普通方法是，锁对象是this； 当synchronized作用于静态方法是，锁对象是当前类的Class对象； 当synchronized作用于代码块时，锁对象是synchronized(obj)中的这个obj。 但是有synchronized无法做到的事情，譬如： 我们想给锁加个等待时间超时时间，超时还未获得锁就放弃，不至于无限等下去； 我们想以可中断的方式获取锁，这样外部线程给我们发一个中断信号就能唤起等待锁的线程； 我们想为锁维持多个等待队列，比如一个生产者队列，一个消费者队列，一边提高锁的效率。 要实现这些功能就需要显式锁 Lock接口 在java SE5 之后，Oracle在并发包中添加了Lock接口，以及相应的实现类，用来实现显式锁功能。此接口提供sychronized关键子类似的同步功能。 Lock接口提供的而synchronized关键子字具备的特性 尝试非阻塞的获取锁 能被中断地获取锁 超时获取锁 Lock是一个接口，它定义了锁获取和释放的基本操作 队列同步器（AQS） Lock接口的实现之一是ReentrantLock，而ReentrantLock继承了队列同步器AbstractQueuedSynchronizer. AQS的主要使用方式是继承，是基于模板方法模式来设计的。使用者需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。 同步状态（state） 在AQS类中存在一个变量state被成为同步状态。 1private volatile int state; 重写同步器指定的方法时，需要使用同步器提供的如下3个方法来访问或修改同步状态。 getState()：获取当前同步状态。 setState(int newState)：设置当前同步状态。 compareAndSetState(int expect,int update)：使用CAS设置当前状态，该方法能够保证状态设置的原子性。 AQS使用CAS对该同步状态进行原子操作实现对其值的修改。在对锁的实现中，通过修改同步状态来得知是否可以加锁或者解锁。 譬如在ReentrantLock中，重写了AQS中的tryAcquire函数 1234567protected final boolean tryAcquire(int acquires) { if (getState() == 0 &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } 通过判断getSate()==0 和 CAS修改同步状态成功来 判断锁可以被获得，且竞争成功 同步器可重写的方法与描述 实现自定义同步组件时，将会调用同步器提供的模板方法 AQS 对资源的共享方式 Exclusive(独占)：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的 Share(共享)：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock。 ReentrantReadWriteLock 可以看成是组合式，因为ReentrantReadWriteLock也就是读写锁允许多个线程同时对某一资源进行读。 队列同步器及其实现 AQS依赖内部的同步队列（一个FIFO双向队列）来完成同步状态的管理。 当线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成一个节点（Node）并将其加入同步队列，同时阻塞当前线程。 在节点Node中有一个重要的信息volatile int waitStatus，Node使用此变量来表示节点的状态 1234567static final int CANCELLED = 1;static final int SIGNAL = -1;static final int CONDITION = -2;static final int PROPAGATE = -3; // 结点状态 volatile int waitStatus; 具体状态如下。 CANCELLED，值为1，表示当前的线程被取消。 SIGNAL，值为-1，表示当前节点的后继节点包含的线程需要运行，需要进行unpark操作。 CONDITION，值为-2，表示当前节点在等待condition，也就是在condition queue中。 PROPAGATE，值为-3，表示当前场景下后续的acquireShared能够得以执行。 值为0，表示当前节点在sync queue中，等待着获取锁。 同步队列由这些节点构成 在线程无法获取同步状态，进入同步队列时，需要保证线程安全。AQS提供基于CAS的设置尾节点的方法：compareAndSetTail(Node expect,Node update) 同步队列遵循FIFO，首节点是获取同步状态成功的节点，首节点的线程在释放同步状态时，将会唤醒后继节点，而后继节点将会在获取同步状态成功时将自己设置为首节点 设置首节点是通过获取同步状态成功的线程来完成的，由于只有一个线程能够成功获取到同步状态，因此设置头节点的方法并不需要使用CAS来保证，它只需要将首节点设置成为原首节点的后继节点并断开原首节点的next引用即可 独占式同步状态的获取和释放（自旋锁） 通过调用同步器的acquire(int arg)方法可以获取同步状态，该方法对中断不敏感，线程获取同步状态失败后进入同步队列中后对线程进行中断时，线程不会从同步队列中移出。 同步器的acquire方法 12345public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; //尝试获取锁 acquireQueued(addWaiter(Node.EXCLUSIVE), arg))//加入同步队列，同时调用acquireQueued selfInterrupt();} 其主要逻辑为：首先调用自定义同步器实现的tryAcquire(int arg)方法，该方法保证线程安全的获取同步状态，如果同步状态获取失败，则构造同步节点（独占式Node.EXCLUSIVE，同一时刻只能有一个线程成功获取同步状态）并通过addWaiter(Node node)方法将该节点加入到同步队列的尾部。 最后调用acquireQueued(Node node,int arg)方法，使得该节点以死循环的方式获取同步状态，只有当成功获取同步状态后，函数才可以退出。如果获取不到则阻塞节点中的线程，而被阻塞线程的唤醒主要依靠前驱节点的出队或阻塞线程被中断来实现。 节点进入同步队列之后，就进入了一个自旋的过程，每个节点（或者说每个线程）都在自省地观察，当条件满足，获取到了同步状态，就可以从这个自旋过程中退出，否则依旧留在这个自旋过程中（并会阻塞节点的线程） 在acquireQueued(final Node node,int arg)方法中，当前线程在“死循环”中尝试获取同步状态，而只有前驱节点是头节点才能够尝试获取同步状态，这是为什么？原因有两个，如下。 第一，头节点是成功获取到同步状态的节点，而头节点的线程释放了同步状态之后，将会唤醒其后继节点，后继节点的线程被唤醒后需要检查自己的前驱节点是否是头节点。 第二，维护同步队列的FIFO原则。该方法中，节点自旋获取同步状态的行为如图所示。 独占式同步状态获取流程，也就是acquire(int arg)方法调用流程，如图所示 共享式同步状态获取与释放 共享式获取与独占式获取最主要的区别在于同一时刻能否有多个线程同时获取到同步状态 通过调用同步器的acquireShared(int arg)方法可以共享式地获取同步状态 与独占式一样，共享式获取也需要释放同步状态，通过调用releaseShared(int arg)方法可以释放同步状态 该方法在释放同步状态之后，将会唤醒后续处于等待状态的节点。对于能够支持多个线程同时访问的并发组件（比如Semaphore），它和独占式主要区别在于tryReleaseShared(int arg)方法必须确保同步状态（或者资源数）线程安全释放，一般是通过循环和CAS来保证的，因为释放同步状态的操作会同时来自多个线程 独占式超时获取同步状态 通过调用同步器的doAcquireNanos(int arg,long nanosTimeout)方法可以超时获取同步状态，即在指定的时间段内获取同步状态，如果获取到同步状态则返回true，否则，返回false。 超时获取同步状态过程可以被视作响应中断获取同步状态过程的“增强版”，doAcquireNanos(int arg,long nanosTimeout)方法在支持响应中断的基础上，增加了超时获取的特性。 重入锁(ReentrantLock) 重入锁ReentrantLock，顾名思义，就是支持重进入的锁，它表示该锁能够支持一个线程对资源的重复加锁。除此之外，该锁的还支持获取锁时的公平和非公平性选择。 可重入的实现 可重入的实现主要解决以下问题 线程再次获取锁：锁需要去识别获取锁的线程是否为当前占据锁的线程，如果是，则再次成功获取。 锁的最终释放：线程重复n次获取了锁，随后在第n次释放该锁后，其他线程能够获取到该锁。锁的最终释放要求锁对于获取进行计数自增，计数表示当前锁被重复获取的次数，而锁被释放时，计数自减，当计数等于0时表示锁已经成功释放。 ReentrantLock是通过组合自定义同步器来实现锁的获取与释放，以非公平性（默认的）实现为例 123456789101112131415161718final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { //通过判断当前线程是否为获取锁的线程来决定获取操作是否成功，如果是获取锁的线程再次请求，则将同步状态值进行增加并返回true，表示获取同步状态成功 int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false;} ReentrantLock在释放同步状态时需要减少同步状态值 123456789101112protected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free;} 公平与非公平获取锁的区别 公平性与否是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求的绝对时间顺序，也就是FIFO。 对于非公平锁，只要CAS设置同步状态成功，则表示当前线程获取了锁，而公平锁则不同，需要判断同步队列中当前节点是否有前驱节点，需要等待前驱线程获取并释放锁之后才能继续获取锁。 读写锁（ReentrantReadWriteLock） 读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量。Java并发包提供读写锁的实现是ReentrantReadWriteLock，它提供的特性如图 读写锁接口 ReadWriteLock仅定义了获取读锁和写锁的两个方法，即readLock()方法和writeLock()方法，而其实现——ReentrantReadWriteLock，除了接口方法之外，还提供了一些便于外界监控其内部工作状态的方法 读写锁实现分析 读写锁使用同步器的同步状态来表示锁的读写状态 对同步状态进行高16为和底16位的切割 写锁的获取与释放 写锁是一个支持重进入的排它锁。如果当前线程已经获取了写锁，则增加写状态。如果当前线程在获取写锁时，读锁已经被获取（读状态不为0）或者该线程不是已经获取写锁的线程，则当前线程进入等待状态。 读锁的获取与释放 读锁是一个支持重进入的共享锁，它能够被多个线程同时获取，在没有其他写线程访问（或者写状态为0）时，读锁总会被成功地获取，而所做的也只是（线程安全的）增加读状态。如果当前线程已经获取了读锁，则增加读状态。如果当前线程在获取读锁时，写锁已被其他线程获取，则进入等待状态 锁降级 锁降级是指把持住（当前拥有的）写锁，再获取到读锁，随后释放（先前拥有的）写锁的过程。 锁降级中读锁的获取是否必要呢？ 答案是必要的。主要是为了保证数据的可见性，如果当前线程不获取读锁而是直接释放写锁，假设此刻另一个线程（记作线程T）获取了写锁并修改了数据，那么当前线程无法感知线程T的数据更新。如果当前线程获取读锁，即遵循锁降级的步骤，则线程T将会被阻塞，直到当前线程使用数据并释放读锁之后，线程T才能获取写锁进行数据更新。 LockSupport工具 当需要阻塞或唤醒一个线程的时候，都会使用LockSupport工具类来完成相应工作。LockSupport定义了一组的公共静态方法，这些方法提供了最基本的线程阻塞和唤醒功能，而LockSupport也成为构建同步组件的基础工具。 阻塞和唤醒方法 Condition接口 Condition接口也提供了类似Object的监视器方法wait()、wait(long timeout)、notify()以及notifyAll()，与Lock配合可以实现等待/通知模式。 Condition特性和接口 获取一个Condition必须通过Lock的newCondition()方法。 Condition实现分析 等待队列的实现 等待队列是一个FIFO队列，在队列中的每个节点都包含了一个线程引用，该线程就是在Condition对象上等待的线程，如果一个线程调用了Condition.await()方法，那么该线程将会释放锁、构造成节点加入等待队列并进入等待状态 一个Condition包含一个等待队列，Condition拥有首节点（firstWaiter）和尾节点（lastWaiter）。当前线程调用Condition.await()方法，将会以当前线程构造节点，并将节点从尾部加入等待队列。 上述节点引用更新的过程并没有使用CAS保证，原因在于调用await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的。 在Object的监视器模型，一个对象拥有一个同步队列和等待队列。 并发包中的Lock（更确切地说是同步器）拥有一个同步队列和多个等待队列 等待 调用Condition的await()方法（或者以await开头的方法），会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。当从await()方法返回时，当前线程一定获取了Condition相关联的锁。 如果从队列（同步队列和等待队列）的角度看await()方法，当调用await()方法时，相当于同步队列的首节点（获取了锁的节点）移动到Condition的等待队列中。 同步队列的首节点并不会直接加入等待队列，而是通过addConditionWaiter()方法把当前线程构造成一个新的节点并将其加入等待队列中 通知 调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中 Condition的signal()方法 调用signal()方法的条件是当前线程必须获取了锁。 节点从等待队列移动到同步队列的过程 通过调用同步器的enq(Node node)方法，等待队列中的头节点线程安全地移动到同步队列。当节点移动到同步队列后，当前线程再使用LockSupport唤醒该节点的线程。 被唤醒后的线程，将从await()方法中的while循环中退出（isOnSyncQueue(Node node)方法返回true，节点已经在同步队列中），进而调用同步器的acquireQueued()方法加入到获取同步状态的竞争中。 成功获取同步状态（或者说锁）之后，被唤醒的线程将从先前调用的await()方法返回，此时该线程已经成功地获取了锁。 Condition的signalAll()方法，相当于对等待队列中的每个节点均执行一次signal()方法，效果就是将等待队列中所有节点全部移动到同步队列中，并唤醒每个节点的线程。 总结 java的显式锁要实现Lock接口。 一般实现Lock接口的显式锁需要继承AQS，并借用AQS中提供的某些方法进行实现。 java中提供的显式锁有重入锁(ReentrantLock)和读写锁(ReentrantReadWriteLock)。 其中线程的阻塞和唤醒需要LockSupport工具中的park()和unpark()函数 线程之间进行通信/等待功能，通过AQS中的ConditionObject，构造等待队列以及实现Condition接口的功能。","link":"cn/java%E5%B9%B6%E5%8F%91(4)/"},{"title":"PriorityQueue学习总结","text":"源码来自与OpenJDK 17 我选择个人认为重要的点进行总结 PriorityQueue继承自AbstractQueue，实现java.io.Serializable接口 12public class PriorityQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements java.io.Serializable 元素存储至一个数组中 1transient Object[] queue; PriorityQueue使用modCount来显示结构的修改次数 1transient int modCount; PriorityQueue使用 add(E e)函数来添加元素 当空间不足时，使用``grow() `函数添加 最终进行元素的插入时，需要使用siftUp(int k, E x)对堆的性质进行维护： 在数组最后插入元素，不断的与其父元素进行比较，并进行相应的元素交换。 1234567891011121314151617181920212223242526272829303132 public boolean add(E e) { return offer(e); } public boolean offer(E e) { if (e == null) throw new NullPointerException(); modCount++; int i = size; if (i &gt;= queue.length) grow(i + 1); siftUp(i, e); size = i + 1; return true; }private void siftUp(int k, E x) { if (comparator != null) siftUpUsingComparator(k, x, queue, comparator); else siftUpComparable(k, x, queue); } private static &lt;T&gt; void siftUpComparable(int k, T x, Object[] es) { Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;) x; while (k &gt; 0) { int parent = (k - 1) &gt;&gt;&gt; 1; Object e = es[parent]; if (key.compareTo((T) e) &gt;= 0) break; es[k] = e; k = parent; } es[k] = key; } PriorityQueue使用poll()或remove()删除堆首元素 删除首元素后，将最后一个元素提到堆首，并使用 siftDownComparable(int k, T x, Object[] es, int n) 不断下沉，进行堆性质的维护 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//AbstractQueue public E remove() { E x = poll(); if (x != null) return x; else throw new NoSuchElementException(); }//PriorityQueue public E poll() { final Object[] es; final E result; if ((result = (E) ((es = queue)[0])) != null) { modCount++; final int n; final E x = (E) es[(n = --size)]; es[n] = null; if (n &gt; 0) { final Comparator&lt;? super E&gt; cmp; if ((cmp = comparator) == null) siftDownComparable(0, x, es, n); else siftDownUsingComparator(0, x, es, n, cmp); } } return result; } private static &lt;T&gt; void siftDownComparable(int k, T x, Object[] es, int n) { // assert n &gt; 0; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;)x; int half = n &gt;&gt;&gt; 1; // loop while a non-leaf while (k &lt; half) { int child = (k &lt;&lt; 1) + 1; // assume left child is least Object c = es[child]; int right = child + 1; if (right &lt; n &amp;&amp; ((Comparable&lt;? super T&gt;) c).compareTo((T) es[right]) &gt; 0) c = es[child = right]; if (key.compareTo((T) c) &lt;= 0) break; es[k] = c; k = child; } es[k] = key; } PriorityQueue 使用 remove(Object o)删除指定的元素 并在删除的同时维护堆的性质 1234567891011121314151617181920212223242526272829public boolean remove(Object o) { int i = indexOf(o); if (i == -1) return false; else { removeAt(i); return true; }}E removeAt(int i) { // assert i &gt;= 0 &amp;&amp; i &lt; size; final Object[] es = queue; modCount++; int s = --size; if (s == i) // removed last element es[i] = null; else { E moved = (E) es[s]; es[s] = null; siftDown(i, moved); if (es[i] == moved) { siftUp(i, moved); if (es[i] != moved) return moved; } } return null;} 在PriorityQueue中建立堆，调用之前不要求元素的排列顺序 1234567891011private void heapify() { final Object[] es = queue; int n = size, i = (n &gt;&gt;&gt; 1) - 1; final Comparator&lt;? super E&gt; cmp; if ((cmp = comparator) == null) for (; i &gt;= 0; i--) siftDownComparable(i, (E) es[i], es, n); else for (; i &gt;= 0; i--) siftDownUsingComparator(i, (E) es[i], es, n, cmp);}","link":"cn/PriorityQueue/"},{"title":"ConcurrentHashMap学习总结","text":"本文内的代码均来自JDK1.8 简介 ConcurrentHashMap类来自java并发工具包JUC。作者是Doug Lea。 ConcurrentHashMap可以在并发的条件下使用，拥有优于HashTable的性能 使用ConcurrentHashMap的好处 在并发的环境下使用HashMap可能会出现问题 在JDK1.7使用头插入的方式，在并发的条件下会导致链表出现环 在JDK1.8中使用尾插入的方式，避免了环的产生，但是并发下会可能产生死锁问题 HashTable的并发性能差 HashTable的实现使用了synchronized关键字对put等操作进行加锁，相当于锁住了整个HashTable对象，效率低下 ConcurrentHashMap的结构 分段锁思想 ConcurrentHashMap使用了分段锁的思想，对整个map对象的上多个锁，每个锁对应一个或多个table数组的位置，从而实现多个线程并发修改的效果。 JDK1.7中的segment结构 在 jdk 1.7 中，ConcurrentHashMap 是由 Segment 数据结构和 HashEntry 数组结构构成。采取分段锁来保证安全性。 segment继承自ReentrantLock，属于重入锁。 一个 ConcurrentHashMap 里包含一个 Segment 数组，一个 Segment 里包含一个 HashEntry 数组，Segment 的结构和 HashMap 类似，是一个数组和链表结构。 使用put操作时，根据hash算法定位到元素的segment，并尝试获取锁。 不同的segment之间可以实现并发操作而不相互影响。 segment默认最大值为16，也就是说，最多支持16个线程进行访问和修改。 Entry的数据结构采用链表作为存储结构。 单个segment对应的Entry数组是可扩容的。 摒弃了segment设计的JDK1.8版本 JDK1.8上的ConcurrentHashMap的数据结构由名为table的node数组构成 1transient volatile Node&lt;K,V&gt;[] table; 每一个table位置上的可以存储一个链表，或者一棵红黑树。 在JDK1.7之前，ConcurrentHashMap是通过分段锁机制来实现的，所以其最大并发度受Segment的个数限制。且当 Hash 冲突严重时，在桶上形成的链表会变的越来越长，这样在查询时的效率就会越来越低；时间复杂度为 O(N)。 在JDK1.8及之后，ConcurrentHashMap中的锁的颗粒度更细了。jdk1.7锁的粒度是基于Segment的，包含多个HashEntry，而jdk1.8锁的粒度就是Node，所以jdk1.8中的concurrentLevel是和数组大小保持一致的，每次扩容，并发度扩大一倍。并且引入了红黑树的结构。 在数据结构上的变动： 摈弃了segment结构，改成了CAS+synchronized+volatile来进行并发读写的实现 储存结构上选择与HashMap相似的table数组+链表+红黑树的方式实现。 为什么不用ReentrantLock而用synchronized 通过 JDK 的源码和官方文档看来， 他们认为的弃用分段锁的原因由以下几点： 如果使用ReentrantLock则需要节点继承AQS来获得同步支持，增加内存开销，而1.8中只有头节点需要进行同步。 生产环境中， map 在放入时竞争同一个锁的概率非常小，分段锁反而会造成更新等操作的长时间等待。 synchronized具有jvm原生支持，效率高 为了提高 GC 的效率 JDK1.8 中的ConcurrentHashMap源码分析 成员变量 1、储存哈希表的数组table，使用volatile保证一致性 1transient volatile Node&lt;K,V&gt;[] table; 2、在扩容的时候使用的数组，扩容时会慢慢把数据移动到这个数组。其余时间为null 1private transient volatile Node&lt;K,V&gt;[] nextTable; 3、在没有发生争用时的元素统计 1private transient volatile long baseCount; 4、扩容索引值,表示已经分配给扩容线程的table数组索引位置,主要用来协调多个线程间迁移任务的并发安全. 1private transient volatile int transferIndex; 5、一个在多线程间共享的竞态变量,用于维护各种状态,保存各类信息 1private transient volatile int sizeCtl; sizeCtl &gt; 0时可分为两种情况: 未初始化时,sizeCtl表示初始容量. 初始化后表示扩容的阈值,为当前数组长度length*0.75 sizeCtl = -1: 表示正在初始化或者扩容阶段. sizeCtl &lt; -1 : sizeCtl承担起了扩容时标识符(高16位)和参与线程数目(低16位)的存储 在addCount和helpTransfer的方法代码中,如果需要帮助扩容,则会CAS替换为sizeCtl+1 在完成当前扩容内容,且没有再分配的区域时,线程会退出扩容,此时会CAS替换为sizeCtl-1 Node 构成了基本的K/V对，实现了Entry接口 12345678910111213141516171819202122232425262728293031323334353637383940414243444546static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.val = val; this.next = next; } public final K getKey() { return key; } public final V getValue() { return val; } public final int hashCode() { return key.hashCode() ^ val.hashCode(); } public final String toString(){ return key + &quot;=&quot; + val; } public final V setValue(V value) { throw new UnsupportedOperationException(); } public final boolean equals(Object o) { Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); } /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) { Node&lt;K,V&gt; e = this; if (k != null) { do { K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; } while ((e = e.next) != null); } return null; }} ConcurrentHashMap特殊点： val值和next值使用volatile 修饰，保证并发可见性 hashCode方法略有不同,因为ConcurrentHashMap不支持key或value为NULL值,所以直接使用key.hashCode() ^ val.hashCode()跳过了为空判断。 ForwardingNode 在转移操作过程中使用的，在桶首位插入的一个节点。 ForwardingNode的hash为-1，是一个辅助类 123456789101112131415161718192021222324252627282930313233 static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; { final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) { super(MOVED, null, null, null); this.nextTable = tab; }// 帮助扩容时的元素查找 Node&lt;K,V&gt; find(int h, Object k) { // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) { Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; for (;;) { int eh; K ek; if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; if (eh &lt; 0) { if (e instanceof ForwardingNode) { tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; } else return e.find(h, k); } if ((e = e.next) == null) return null; } } } } TreeNode 红黑树的节点类，继承自Node 重写了find函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; { TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) { super(hash, key, val, next); this.parent = parent; } Node&lt;K,V&gt; find(int h, Object k) { return findTreeNode(h, k, null); } /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ final TreeNode&lt;K,V&gt; findTreeNode(int h, Object k, Class&lt;?&gt; kc) { if (k != null) { TreeNode&lt;K,V&gt; p = this; do { int ph, dir; K pk; TreeNode&lt;K,V&gt; q; TreeNode&lt;K,V&gt; pl = p.left, pr = p.right; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.findTreeNode(h, k, kc)) != null) return q; else p = pl; } while (p != null); } return null; }} CocurrentHashMap初始化 初始化一个合适大小的数组，然后会设置 sizeCtl。 初始化方法中的并发问题是通过对 sizeCtl 进行一个 CAS 操作来控制的 123456789101112131415161718192021222324252627282930private final Node&lt;K,V&gt;[] initTable() { Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) { // 初始化的&quot;功劳&quot;被其他线程&quot;抢去&quot;了 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS 一下，将 sizeCtl 设置为 -1，代表抢到了锁 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { // DEFAULT_CAPACITY 默认初始容量是 16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组，长度为 16 或初始化时提供的长度 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 将这个数组赋值给 table，table 是 volatile 的 table = tab = nt; // 如果 n 为 16 的话，那么这里 sc = 12 // 其实就是 0.75 * n sc = n - (n &gt;&gt;&gt; 2); } } finally { // 设置 sizeCtl 为 sc，我们就当是 12 吧 sizeCtl = sc; } break; } } return tab;} 在初始化成功后，第一次使用put函数时，会对table进行资源的分配。 元素的获得 ——get函数 1234567891011121314151617181920public V get(Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode());//使用spread函数对hash值进行处理 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) {//相应的桶存在数据 if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; } // 如果头结点的 hash 小于 0，说明 正在扩容，或者该位置是红黑树 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) {//链表遍历查询 if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; } } return null;} get 过程分析 计算 hash 值 根据 hash 值找到数组对应位置: (n - 1) &amp; h 根据该位置处结点性质进行相应查找 如果该位置为 null，那么直接返回 null 就可以了 如果该位置处的节点刚好就是我们需要的，返回该节点的值即可 如果该位置节点的 hash 值小于 0，说明正在扩容，或者是红黑树，后面我们再介绍 find 方法 如果以上 3 条都不满足，那就是链表，进行遍历比对即可 get函数不加锁的原因 Node的成员val是用volatile修饰的 volatile可以保证元素的可见性 元素的添加——put函数 过程分析： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104/** * 方法参数: * 1. key,value 自然不用说就是k/v的两个值 * 2. onlyIfAbsent 若为true,则仅仅在值为空时覆盖 * 返回值: * 返回旧值,若是新增就为null. */final V putVal(K key, V value, boolean onlyIfAbsent) { // CHM不支持NULL值的铁证. if (key == null || value == null) throw new NullPointerException(); // 获得key的Hash,spread可以称之为扰动函数 int hash = spread(key.hashCode()); int binCount = 0; // 无限循环 for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; // 在tab为空时负责初始化Table if (tab == null || (n = tab.length) == 0) tab = initTable(); // 使用`(n-1)&amp;hash`确定了元素的下标位置,获取对应节点 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { // 如果对应位置节点为空,直接以当前信息为桶的头节点 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } // 如果获取的桶的头结点的`Hash`为`MOVED`,表示该节点是`ForwardingNode` // 也就表示数组正在进行扩容 else if ((fh = f.hash) == MOVED) // 帮助扩容 tab = helpTransfer(tab, f); else { V oldVal = null; // 上锁保证原子性,volatile仅能保证可见性 // f为key获取到的节点元素,以此为锁对象 synchronized (f) { // f在上文就是根据`tabAt(tab,i)`获取的 // 此处是再次获取验证有没有被修改 if (tabAt(tab, i) == f) { // 与else.if比较,得知 // fh &gt;= 0表示当前节点为链表节点,即当前桶结构为链表 if (fh &gt;= 0) { // 链表中的元素个数统计 binCount = 1; // 循环遍历整个桶 // 跳出循环的两种情况: // 1. 找到相同的值,binCount此时表示遍历的节点个数 // 2. 遍历到末尾,binCount就表示桶中的节点个数 for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; // 源码中大量运用了表达式的短路特性,来展示判断的优先级 // 1. 若hash不相等,则直接跳过判断 // 2. hash相等之后,若key的地址相同,则直接进入if // 3. 地址不同时在进入判断内容是否相等 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; // onlyIfAbsent为true,表示存在时不覆盖内容 if (!onlyIfAbsent) e.val = value; // 已经找到确定的元素了,更新不更新都跳出 break; } // 因为e就在同步代码块中,桶已经被上锁,不可能有别的线程改变 // 所以不需要重新获取 Node&lt;K,V&gt; pred = e; // 1. 如果e为空,则直接将元素挂接到e后面,跳出循环 // 2. e不为空,继续遍历 if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } // 类似HashMap,树节点独立操作. else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } // 表示进入了上面的同步表达式,对桶进行修改之后 if (binCount != 0) { // 如果binCount大于树的临界值,就将链表转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); // 如果oldVal部位空,则返回 if (oldVal != null) return oldVal; break; } } } // 添加元素计数,并在binCount大于0时检查是否需要扩容 addCount(1L, binCount); return null;} put过程 整体流程跟HashMap比较类似，大致是以下几步： 如果桶数组未初始化，则初始化； 如果待插入的元素所在的桶为空，则尝试把此元素直接插入到桶的第一个位置； 如果正在扩容，则当前线程一起加入到扩容的过程中； 如果待插入的元素所在的桶不为空且不在迁移元素，则锁住这个桶（分段锁）； 如果当前桶中元素以链表方式存储，则在链表中寻找该元素或者插入元素； 如果当前桶中元素以红黑树方式存储，则在红黑树中寻找该元素或者插入元素； 如果元素存在，则返回旧值； 如果元素不存在，整个Map的元素个数加1，并检查是否需要扩容； 添加元素操作中使用的锁主要有（自旋锁 + CAS + synchronized + 分段锁）。 扩容和转移 addCount 作用：增加元素统计的个数 其中调用了fullAddCount函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157/** * 参数: * x -&gt; 具体增加的元素个数 * check -&gt; 如果check&lt;0不检查时都需要扩容, */private final void addCount(long x, int check) { CounterCell[] as; long b, s; // 1. counterCells不为空 // 2. CAS修改baseCount属性成功 if ((as = counterCells) != null || // CAS增加baseCount !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) { CounterCell a; long v; int m; // 线程争用的状态标记 boolean uncontended = true; // 1. 计数cell为null,或长度小于1 // 2. 随机去一个数组位置为为空 // 3. CAS替换CounterCell的value失败 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || // CAS增加CounterCell的value值失败会调用fullAddCount方法 !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) { fullAddCount(x, uncontended); return; } if (check &lt;= 1) return; s = sumCount(); } // 根据`check &gt;= 0`判断是否需要检查扩容 if (check &gt;= 0) { Node&lt;K,V&gt;[] tab, nt; int n, sc; // 1. 如果元素总数大于sizeCtl,表示达到了扩容阈值 // 2. tab数组不能为空,已经初始化 // 3. table.length小于最大容,有扩容空间 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) { // 根据数组长度获取一个扩容标志 int rs = resizeStamp(n); if (sc &lt; 0) { // 如果sc的低16位不等于rs,表示标识符已经改变. // 如果nextTable为空,表示扩容已经结束 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // CAS替换sc值为sc+1,成功则开始扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) // 调用transfer开始扩容,此时nextTable已经指定 transfer(tab, nt); } // `sc &gt; 0`表示数组此时并不在扩容阶段,更新sizeCtl并开始扩容 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) // 调用transfer,nextTable待生成 transfer(tab, null); s = sumCount(); } } }private final void fullAddCount(long x, boolean wasUncontended) { int h; //获取当前线程的probe的值，如果值为0，则初始化当前线程的probe的值,probe就是随机数 if ((h = ThreadLocalRandom.getProbe()) == 0) { ThreadLocalRandom.localInit(); // force initialization h = ThreadLocalRandom.getProbe(); wasUncontended = true; // 由于重新生成了probe，未冲突标志位设置为true } boolean collide = false; // True if last slot nonempty for (;;) {//自旋 CounterCell[] as; CounterCell a; int n; long v; // 说明counterCells已经被初始化过了，我们先跳过这个代码，先看初始化部分 if ((as = counterCells) != null &amp;&amp; (n = as.length) &gt; 0) { if ((a = as[(n - 1) &amp; h]) == null) { // 通过该值与当前线程probe求与， //获得cells的下标元素，和hash 表获取索引是一样的 if (cellsBusy == 0) { // cellsBusy=0表示counterCells不在初始化或者扩容状态下 CounterCell r = new CounterCell(x); // 构造一个CounterCell的值，传入元素个数 if (cellsBusy == 0 &amp;&amp;// 通过cas设置cellsBusy标识，防止其他线程来对counterCells并发处理 U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { boolean created = false; try { // Recheck under lock CounterCell[] rs; int m, j; // 将初始化的r对象的元素个数放在对应下标的位置 if ((rs = counterCells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null){ rs[j] = r; created = true; } } finally {//恢复标志位 cellsBusy = 0; } if (created)//创建成功，退出循环 break; continue;//说明指定cells下标位置的数据不为空，则进行下一次循环 } } collide = false; } //说明在addCount方法中cas失败了，并且获取probe的值不为空 else if (!wasUncontended) // CAS already known to fail wasUncontended = true; //设置为未冲突标识，进入下一次自旋 //由于指定下标位置的cell值不为空，则直接通过cas进行原子累加，如果成功，则直接退出 else if (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))// break; // 如果已经有其他线程建立了新的counterCells或者CounterCells大于CPU核心数 //（很巧妙，线程的并发数不会超过cpu核心数） else if (counterCells != as || n &gt;= NCPU) collide = false; //设置当前线程的循环失败不进行扩容 else if (!collide)//恢复collide状态，标识下次循环会进行扩容 collide = true; //进入这个步骤，说明CounterCell数组容量不够，线程竞争较大，所以先设置一个标识表示为正在扩容 else if (cellsBusy == 0 &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { try { if (counterCells == as) {// Expand table unless stale //扩容一倍 2变成4，这个扩容比较简单 CounterCell[] rs = new CounterCell[n &lt;&lt; 1]; for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; counterCells = rs; } } finally { cellsBusy = 0;//恢复标识 } collide = false; continue;//继续下一次自旋 } h = ThreadLocalRandom.advanceProbe(h);//更新随机数的值 } //初始化 CounterCells 数组 //cellsBusy=0表示没有在做初始化，通过cas更新cellsbusy的值标注当前线程正在做初 始化操作 else if (cellsBusy == 0 &amp;&amp; counterCells == as &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { boolean init = false; try { // Initialize table if (counterCells == as) { CounterCell[] rs = new CounterCell[2]; //初始化容量为2 rs[h &amp; 1] = new CounterCell(x);//将x也就是元素的个数放在指定的数组 下标位置 counterCells = rs;//赋值给counterCells init = true;//设置初始化完成标识 } } finally { cellsBusy = 0;//恢复标识 } if (init) break; } //竞争激烈，其它线程占据cell 数组，直接累加在base变量中 else if (U.compareAndSwapLong(this, BASECOUNT, v = baseCount, v + x)) break; // Fall back on using base } } helpTransfer 作用： 在put中调用并帮助扩容 具体通过调用transfer进行扩容 123456789101112131415161718192021222324252627282930313233343536373839404142434445 /** * 参数： * tab -&gt; 扩容的数组，一般为table * f -&gt; 线程持有的锁对应的桶的头节点 * 调用地方: * 1. `putVal`检测到头节点Hash为MOVED */final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) { Node&lt;K,V&gt;[] nextTab; int sc; // 1.参数数组不能为空 // 2.参数f必须为ForwardingNode类型 // 3.f.nextTab不能为空 if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) { // resizeStamp一顿位操作打的我头昏脑涨 // 获取扩容的标识 int rs = resizeStamp(tab.length); // Map仍处在扩容状态的判断 // 1. 判断节点f的nextTable是否和成员变量的nextTable相同 // 2. 判断传入的tab和成员变量的table是否相同 // 3. sizeCtl是否小于0 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) { // 两种不同的情况判断 // 一. 不需要帮助扩容的情况 // 1. sc的高16位不等于rs // 2. sc等于rs+1 // 3. sc等于rs+MAX_RESIZERS // 4. transferIndex &lt;= 0, 这个好理解因为扩容时会分配并减去transferIndex, // 小于0时表示数组的区域已分配完毕 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; // 二. CAS `sc+1`并调用transfer帮助扩容. // 线程在帮助扩容时会对sizeCtl+1,完成时-1,表示标记 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) { transfer(tab, nextTab); break; } } return nextTab; } return table; } transfer 在扩容时转移元素的函数 将一个大的迁移任务分为了一个个任务包 此方法支持多线程执行，外围调用此方法的时候，会保证第一个发起数据迁移的线程，nextTab 参数为 null，之后再调用此方法的时候，nextTab 不会为 null。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212// tab是旧表，nextTab是一个两倍容量的空的新表private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { // stide是步长的意思，会将旧表按照这个步长进行分段，在并发扩容时，每个线程只负责自己段内数据的转移 int n = tab.length, stride; // NCPU是当前系统的内核数，如果内核数只有一个，那么就不需要进行并发扩容 // 因为扩容是个纯计算密集型的逻辑，只有一个核心的时候反而得不偿失 // 因为无法真正的并发，反而会额外付出线程上下文切换的开销 // 这里步长最小是16，也就是说每个线程最少要负责16个桶的数据迁移，这个值设置的太小会导致并发线程数增多 // 从而导致线程间的竞争变大，这个竞争是只下面的一些CAS逻辑，比如对transferIndex、sizeCtl变量的cas操作 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) { // initiating try { @SuppressWarnings(&quot;unchecked&quot;) // 如果新表没有初始化，则新建一个双倍容量的新表 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; } catch (Throwable ex) { // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; } nextTable = nextTab; // 设置【开始数据转移】的桶的下标，从尾部的桶开始往头部逐个处理 // 将transferIndex设置为老表的length（比最后一个桶的下标大1，所以后面的代码会-1） transferIndex = n; } int nextn = nextTab.length; // 生成用于表示【扩容中】状态的节点 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // 线程每次根据步长从数组上截取一段桶，如果线程处理完自己截取的一段内的桶后 // 还有未处理的数据，则需要重新从数组上截取一段来处理 // true则标识当前线程需要继续在老表的数组上截取新的一段桶来处理数据 boolean advance = true; // 标记是否已结束扩容，做收尾工作 boolean finishing = false; // to ensure sweep before committing nextTab // i是当前线程需要转移的桶的下标，bound是当前线程负责的桶的最小下标 for (int i = 0, bound = 0;;) { Node&lt;K,V&gt; f; int fh; // 这个while的逻辑就是为了检查当前线程负责的段内的桶是否都处理完毕 // 如果处理完毕，则看下老表的数据里是否还有未处理的桶，如果有未处理的桶，则再次截取一段来处理 while (advance) { int nextIndex, nextBound; // 如果下一个桶的下标（--i是下一个需要操作的桶的下标）还在自己负责的段内 // 就不需要截取新段了，就继续处理下一个桶的数据 // 如果已经结束，则不需要继续截取新的段 if (--i &gt;= bound || finishing) advance = false; // transferIndex用来表示这个下标及其后面的桶都已经被其他线程处理了 //新的线程需要从transferIndex往前截取自己需要负责的桶 // 如果transferIndex小于等于0说明桶都已经转移完毕，不需要再处理了 else if ((nextIndex = transferIndex) &lt;= 0) { i = -1; advance = false; } // 以nextIndex（在上面已经赋值为transferIndex）为起始位置 // 往数组头部方向截取相应步长的段来转移数据，通过cas将transferIndex设置到新的下标 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { // cas成功后，设置当前线程负责的下标边界（比如负责下标32到48的桶，那么这个bound就是32） bound = nextBound; // cas成功后，设置当前线程开始处理的桶的下标（比如负责下标32到48的桶，那么这个i就是48） // transferIndex默认是从tab.length开始取值，所以要减1来表示正确的下标 i = nextIndex - 1; // cas成功则表示当前线程已经成功截取了自己需要负责的一段数据了，不需要再往前截取了 advance = false; } } // i是需要转移的桶的下标，n是老表的容量 // i&lt;0说明旧表中的桶都已经转移完毕 // i&gt;=n|| i + n &gt;= nextn 不是很明白这个判断条件 // 正常情况下，i作为开始转移的桶的下标肯定会小于老表的容量的，因为转移的是老表内的桶 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { int sc; // 判断是否已经完成扩容，已完成扩容则做收尾逻辑 if (finishing) { // 完成扩容后，将引用设置为null nextTable = null; // 将table引用指向新表，这里的table是个volatile变量，所以这个赋值操作对其他线程是可见的 table = nextTab; // 设置新的扩容阈值，将阈值设置为新容量的3/4 // 这里的n是老表的容量，因为是双倍扩容，所以新表容量是2n // 下面计算的结果是2n-0.5n = 1.5n，也就是新表容量的3/4 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); // 返回结果，扩容结束 return; } // 在扩容开始时，会将sizeCtl设置成一个负数，每次有新的线程并发扩容时 // 会将sizeCtl+1，而当有线程处理完扩容逻辑后，再减1，以此来判断是否是最后一个线程 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // cas成功，则判断当前线程是不是最后一个完成扩容的线程 // 由最后一个完成扩容逻辑的线程将finishing和advance设为true，重新循环到上面的if(finishing)里的收尾逻辑 // 这里减2是因为在执行扩容的入口处，第一个触发扩容的线程会负责将sc加2 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit } } // 如果是空桶，则在老表对应的下标出放一个ForwardingNode // 在有别的线程往这个空桶写数据时会感知到扩容过程，一起来扩容 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // ForwardingNode节点的hash值就是MOVED（在ForwardingNode的构造方法里会设置hash值为MOVED） // 说明已经有线程处理了这个桶内的数据 else if ((fh = f.hash) == MOVED) advance = true; // already processed else { // 在桶上加锁，防止对同一个桶数据并发操作 synchronized (f) { // 有点双重校验锁的味道，防止获得锁后，该桶内数据被别的线程插入了新的数据 // 因为这个f是在未加锁之前获取的node对象，在这期间，可能这个下标处插入了新数据 // 比如有别的线程调用了put方法往这个桶内链表插入新节点 // 这时候这个桶的node就变成了新插入的数据node（put操作会生成新的node，并将新node的next引用指向原node） // 如果不做这层校验，会导致新加入到桶内的数据没有被处理，导致数据丢失 if (tabAt(tab, i) == f) { Node&lt;K,V&gt; ln, hn; // fh&gt;=0表示是正常的链表 if (fh &gt;= 0) { // 这里需要注意的是在put操作里是通过hash&amp;(n-1)来选取下标位置，表容量n都是2的幂 // 所以这里hash&amp;n的结果只有两个要么是n要么是0 // 值为0时的节点在新表的i下标出，而值为n的节点需要迁移到新表的i+n下标下，因为是双倍扩容 // 所以老表下标为i的桶内的数据在迁移rehash时 // 一半仍然在下标为i的桶内，另一半在i+n的桶内，不会出现第三种情况 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; // 这个循环的目的有两个 // 1、遍历出整个链表尾部不需要改变next指针的最长链 // 这样可以将这个链整个搬到新桶内，不用再逐个遍历了 // 2、由于是将老的完整节点链条搬到新桶内 // 所以也就不需要创建新的node节点，减少迁移过程中的gc压力 for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; // 逐个遍历节点，0表示仍然放到下标为i的桶内的链表 // 这里每次都是生成新的node对象而不修改原node对象的next指针 // 这也是get()方法不用加锁的关键所在 // 但是会带来gc压力，所以才有上面的那次遍历，希望减少对象的创建 if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); // 否则就是放到下标为i+n的桶内的链表 else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); } // 设置新表的下标为i的桶内数据 setTabAt(nextTab, i, ln); // 设置新表的下标为i+n的桶内数据 setTabAt(nextTab, i + n, hn); // 将老表下标为i的桶内放上ForwardingNode对象，用来标识当前处于扩容过程 setTabAt(tab, i, fwd); // 处理完后，将这个字段设置为true // 以便走到上面的while(advance)里检查当前线程负责的数据是否处理完成，并且查看是否需要截取新段 advance = true; } // 红黑树结构的迁移，逻辑与链表差不多，也是将整棵树拆成两颗 // 一棵树放到下标为i的桶内，一棵放到下标i+n的桶内 else if (f instanceof TreeBin) { TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } } } } } } transfer 这个方法并没有实现所有的迁移任务，每次调用这个方法只实现了 transferIndex 往前 stride 个位置的迁移工作，其他的需要由外围来控制。 tryPresize putAll函数和TreeifyBin函数将会调用此函数。 用于尝试预先调整表格以容纳给定数量的元素。 1234567891011121314151617181920212223242526272829303132333435363738// 首先要说明的是，方法参数 size 传进来的时候就已经翻了倍了private final void tryPresize(int size) { // 根据 size 计算扩容的容量 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; // 判断是否可以进行扩容，如果 sizeCtl &lt;= 0，说明已经在扩容中，那么就不会再进行扩容 while ((sc = sizeCtl) &gt;= 0) { Node&lt;K,V&gt;[] tab = table; int n; // 如果当前容器还没有初始化，则进行初始化，与 initTable 相同 if (tab == null || (n = tab.length) == 0) { // 当前的扩容阀值与传入的值之间选大的作为这次初始化的大小 n = (sc &gt; c) ? sc : c; // 进入初始化状态 if (U.compareAndSetInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); // 相当于 n * 0.75 } } finally { sizeCtl = sc; } } } // 如果还每达到扩容的阀值或者超过了最大容量，则停止扩容 else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; else if (tab == table) { // 开始进行扩容 int rs = resizeStamp(n); if (U.compareAndSetInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); } }} 那些ConcurrentHashMap中的通用工具 spread 扰动函数 123static final int spread(int h) { return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;} 扰动函数,和HashMap中的hash()方法功能类似. CHM中的扰动函数除了将高16位于低16位异或之外又与上HASH_BITS,可以有效降低哈希冲突的概率,使元素分散更加均匀. CAS方法 tabAt 以Volatile方式获取数组元素 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) { return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE); } casTabAt 以CAS形式替换数组元素 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) { return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); } setTabAt 以volatile方式更新数组元素 123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) { U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v); } 参考文章 https://www.cnblogs.com/zerotomax/p/8687425.html https://juejin.cn/post/6875651142958153741 https://github.com/TFdream/blog/wiki/为什么ConcurrentHashMap的读操作不需要加锁？ https://juejin.cn/post/6844903758690779149 https://www.pdai.tech/md/java/thread/java-thread-x-juc-collection-ConcurrentHashMap.html https://juejin.cn/post/6844903763103186958 https://blog.csdn.net/xt199711/article/details/114339022 https://juejin.cn/post/6844903573973630984 https://www.jianshu.com/p/4930801e23c8 https://juejin.cn/post/6844904018729254926 https://juejin.cn/post/6844903678491492359","link":"cn/java%E5%B9%B6%E5%8F%91(5)/"},{"title":"ConcurrentLinkedQueue学习总结","text":"第一次见识使用CAS的无锁并发结构，大为赞叹，妙啊~ 简介 在软件设计中，常常会碰到在并发的情况下使用队列。 在java中提供的安全队列有Vector，Collections.synchronizedList,以及ConcurrentLinkedQueue等，供我们选择 数据结构 ConcurrentLinkedQueue基于链接节点的无界线程安全队列，按照FIFO原则对元素进行排序，加入队列的元素不允许为null ConcurrentLinkedQueue内部结构为实现一个FIFO队列，使用单链表的结构进行连接。类内部存在head和tail对象来记录头节点和尾节点（尾节点其实不一定是尾节点）。 ConcurrentLinkedQueue的数据结构如下： head和tail节点都是由volatile修饰的，具有一致性。 12private transient volatile Node&lt;E&gt; tail;private transient volatile Node&lt;E&gt; head; 由数据节点Node的定义可知，节点所携带的item，以及 next 对象也是volatile修饰的。 123456789101112131415161718192021222324252627282930313233343536373839private static class Node&lt;E&gt; { volatile E item; volatile Node&lt;E&gt; next; Node(E item) { UNSAFE.putObject(this, itemOffset, item); } boolean casItem(E cmp, E val) { return UNSAFE.compareAndSwapObject(this, itemOffset, cmp, val); } void lazySetNext(Node&lt;E&gt; val) { UNSAFE.putOrderedObject(this, nextOffset, val); } boolean casNext(Node&lt;E&gt; cmp, Node&lt;E&gt; val) { return UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val); } // Unsafe mechanics private static final sun.misc.Unsafe UNSAFE; private static final long itemOffset; private static final long nextOffset; static { try { UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = Node.class; itemOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;item&quot;)); nextOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;next&quot;)); } catch (Exception e) { throw new Error(e); } } } 非阻塞算法 阻塞、无锁与无等待的区别 阻塞，blocking 一个方法被称为阻塞的，即这个方法在其演进过程中不能正常运行直到其他（占有锁的）线程释放。 也就是普通的synchronized方法，同一时间内只有一个线程占有了锁，其他线程只能等待。但是这个占有锁的线程并不一定真的在工作（可能被挂起了）。 无锁，lock-free 允许单个线程饿死，但保证系统范围的吞吐量.如果所有线程运行了足够长时间后，至少有一个线程能获得进展，那么这个算法是无锁的。 系统中无论何时，始终有一个线程在工作。 无等待，wait-free 假如一个方法是无等待的，那么它保证了每一次调用都可以在有限的步骤内结束。 系统中的所有线程，都会在有限时间内结束，无论如何也不可能出现饿死(starving)的情况。 Concurrent采用了wait-free算法进行实现，基于Michael &amp; Scott算法的基础上做了一些修改的。 使用CAS来保证并发下的正确性 ConcurrentLinkedQueue采用一系列CAS操作来保证并发下队列运行的正确性 1234567private boolean casTail(Node&lt;E&gt; cmp, Node&lt;E&gt; val) { return UNSAFE.compareAndSwapObject(this, tailOffset, cmp, val);}private boolean casHead(Node&lt;E&gt; cmp, Node&lt;E&gt; val) { return UNSAFE.compareAndSwapObject(this, headOffset, cmp, val);} 以及Node中使用的 1234567891011boolean casItem(E cmp, E val) { return UNSAFE.compareAndSwapObject(this, itemOffset, cmp, val); } void lazySetNext(Node&lt;E&gt; val) { UNSAFE.putOrderedObject(this, nextOffset, val); } boolean casNext(Node&lt;E&gt; cmp, Node&lt;E&gt; val) { return UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val); } 这些CAS的操作最终都会调用UNSAFE类中的native方法实现CAS 关于ABA问题 源算法中CAS 在更新某个指针的时候，compare 的是一个地址，可能出现 compare 成功但是该地址指向的对象其实已经变化。原算法中使用计数器来解决ABA问题。 但是因为有 GC 的存在，一个内存地址能被复用，一定是内存里没有任何引用能指向该地址之前存放的对象。所以，CAS算法本身需要一个指向对象的指针，所以不会改变指向的对象。 具体来说，比如 ConcurrentLinkedQueue 里有很多 casTail(A, B) 即 tail 是否还指向 A 是的话改 tail 指向 B。队列内 Node 都是有方向的，并且每次入队新元素就会重新分配内存生成新 Node，所以 tail 不可能开始指向 A 后来又改到 C 又改回 A，如果出现了意味着 A 入队后 (从 tail 入队) 又入队了 C，结果 A (不是值，是 Node) 又入队了，而这是不可能的。 具体源码实现 入队(offer函数) 注意的点： tail不总是尾节点 CAS自旋的设计 1234567891011121314151617181920212223242526272829303132333435public boolean offer(E e) { checkNotNull(e); final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); // t 指向老的 tail，p 指向最新的 tail，q 指向 tail 的 next for (Node&lt;E&gt; t = tail, p = t;;) { Node&lt;E&gt; q = p.next; if (q == null) { // 如果 q 是空则 p 指向的是实际队尾 if (p.casNext(null, newNode)) { // CAS 的更新队尾的 next 指针，指向新入队 Node // 如果成功，则说明入队成功，因为 tail 是每入队两次才更新 // 所以只有当前 tail 和 old tail 不等时候才会更新 tail if (p != t) // hop two nodes at a time // 即使更新失败也没事，说明有别的线程已经更新了 tail casTail(t, newNode); // Failure is OK. return true; } // Lost CAS race to another thread; re-read next } else if (p == q) // tail 的 next 指向自己，说明 tail 已经被出队，接着读一下最新 tail // 看最新的 tail 和循环开始缓存在 t 的 tail 是否相等 // 相等则说明新读的这个 tail 指针也被出队了，我们只有从 head 一步一步循环去找 tail // 如果不等则说明 tail 已经被其它线程更新了，我们将新的 tail 再缓存在 t 再重来一次 p = (t != (t = tail)) ? t : head; else p = (p != t &amp;&amp; t != (t = tail)) ? t : q; //如果 p 和 t 相等，说明 p 在循环开始后还未向后挪动过，那我们就让 p 向后挪一格 //如果 p 和 t 不相等，则说明 p 已经向后移动过一次，且其他线程在争抢入队成功 //之后若t!=tail则tail已被修改过，设置p为tail， //若t==tail,则未被修改，则可能是tail后有两个node并且未来得及修改 //或者tail后只有一个node不需要修改，这种情况p向后移动 }} 下图是我转载的对offer函数的各种情况的分解 单线程下的offer函数 多线程下的offer函数 出队(poll函数) 注意的点： 注意CAS自旋设计 head节点的item不一定为null 12345678910111213141516171819202122232425262728293031323334public E poll() { restartFromHead: for (;;) { // 因为 head 可能未被更新，可能被出队，所以从 head 开始用循环方式开始出队 Node // h 指向老的 head，p 指向当前认为的 head，q 指向 p 的 next for (Node&lt;E&gt; h = head, p = h, q;;) { E item = p.item; // 如果 item 非 null 说明 head 指向的 Node 还未出队 // CAS 的将 item 设置为 null，哪个线程成功执行 CAS 就是哪个线程成功执行出队 if (item != null &amp;&amp; p.casItem(item, null)) { // 出队成功后判断 p 是否向后移动过，没移动则不更新 head // 因为每出队两个 Node 才更新一次 head if (p != h) // hop two nodes at a time // 注意看到与 LinkedBlokcingQueue 不同 这里出队的可能是 head // 指向的节点而不一定是 head 的 next // 更新 head 时候需要判断队列是否为空，不空时能将 head 直接更新到 p.next updateHead(h, ((q = p.next) != null) ? q : p); return item; } // 队列为空，则尝试更新 head 为 p 后直接返回 null else if ((q = p.next) == null) { updateHead(h, p); return null; } else if (p == q) // p 的 next 指向自己说明 head 被出队需要再读 head 从头开始 continue restartFromHead; else // 走到这里说明出队 head 竞争失败，并且队列非空且当前 p 指向的节点还在队列上 // 所以尝试沿着 next 往后走一格继续尝试出队 p = q; } }} 下图为转载的步骤分析： pull和offer混合并发的情况 为何要采用HOPS(延迟更新的策略)的设计 让tail节点永远作为队列的尾节点，这样实现代码量非常少，而且逻辑清晰和易懂。但是，这么做有个缺点，每次都需要使用循环CAS更新tail节点。如果能减少CAS更新tail节点的次数，就能提高入队的效率，所以doug lea使用hops变量来控制并减少tail节点的更新频率，并不是每次节点入队后都将tail节点更新成尾节点，而是当tail节点和尾节点的距离大于等于常量HOPS的值（默认等于1）时才更新tail节点，tail和尾节点的距离越长，使用CAS更新tail节点的次数就会越少，但是距离越长带来的负面效果就是每次入队时定位尾节点的时间就越长，因为循环体需要多循环一次来定位出尾节点，但是这样仍然能提高入队的效率，因为从本质上来看它通过增加对volatile变量的读操作来减少对volatile变量的写操作，而对volatile变量的写操作开销要远远大于读操作，所以入队效率会有所提升。 特点总结 无界，有 OOM 风险； 无锁，并发性能很高； 没有阻塞接口，不可能阻塞到等待元素到来； 无法获取队列长度精确值； 参考资料 https://ylgrgyq.com/concurrent-linked-queue.html https://en.wikipedia.org/wiki/Non-blocking_algorithm https://www.cnblogs.com/stevenczp/p/6611041.html 《并发编程的艺术》","link":"cn/java%E5%B9%B6%E5%8F%91(6)/"},{"title":"Java线程池初探","text":"简介 在日常的程序编写中，经常会用到一种思想——池化，也就是将资源整合成一个资源池并作统一的分配。在此文章中，线程池就是这一思想的具体体现。 在普通的线程创建中，是直接创建线程，会存在 频繁申请销毁资源 调度资源造成额外的损耗 无限申请资源和缺少内部资源管理等问题 为解决以上问题，采用池化的思想，将线程统一管理。使得单一线程可以在执行完一个任务后而不被销毁，并继续执行下一个任务。 在java中提供了一种线程池框架，Executor框架 线程池的设计 线程池的基本处理流程如下 线程池判断核心线程是否都在执行任务。若不是，则创建一个新的工作线程来执行任务。若核心线程池里的线程都在执行任务，则进入2 线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入3。 线程池判断线程池的所有线程是否都处于工作状态。如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。 以上流程如图所示： java中线程池的实现——Executor框架 Executor框架的主要继承和实现 从此图中可知，Executor框架的实现包含ThreadPoolExecutor、DelegatedExecutorServoce、ForkJoinPool等，他们都实现了ExecutorService接口 顶层接口Executor提供了一种思想：将任务提交和任务执行进行解耦。用户无需关注如何创建线程，如何调度线程来执行任务，用户只需提供Runnable对象，将任务的运行逻辑提交到执行器(Executor)中，由Executor框架完成线程的调配和任务的执行部分。 ExecutorService为Executor补充了可以为一个或一批异步任务生成Future的方法，以及管理线程池的方法。 ExecutorService接口定义的特定行为： execute(Runnable command)：履行Ruannable类型的任务； submit(task)：可用来提交Callable或Runnable任务，并返回代表此任务的Future对象； shutdown()：在完成已提交的任务后封闭办事，不再接管新任务； shutdownNow()：停止所有正在履行的任务并封闭办事； isTerminated()：测试是否所有任务都履行完毕了； isShutdown()：测试是否该ExecutorService已被关闭; awaitTermination(long,TimeUnit)：接收timeout和TimeUnit两个参数，用于设定超时时间及单位。当等待超过设定时间时，会监测ExecutorService是否已经关闭，若关闭则返回true，否则返回false。一般情况下会和shutdown方法组合使用; invokeAll ：作用是等待所有的任务执行完成后统一返回; invokeAny ：将第一个得到的结果作为返回值，然后立刻终止所有的线程。如果设置了超时时间，未超时完成则正常返回结果，如果超时未完成则报超时异常。 在使用时，只需要看接口的方法说明就好，但是在实际的分析中，需要着眼于其中一个常用实现进行具体的分析和理解 线程池的具体实现原理——ThreadPoolExecutor 生产者消费者模型 线程池的内部定义了一个生产者消费者模型，将线程和任务两者解耦，并不直接关联，从而良好的缓冲任务，复用线程。 线程池主要分为两大功能模块，任务管理和线程管理。 任务管理部分充当生产者角色，在用户提交任务后，执行任务的流转： 直接申请线程执行任务 加入缓冲队列 拒绝任务 线程管理部分是消费者，被统一维护在线程池内，根据请求对线程进行分配，或者对线程进行回收。 线程池运行机制如图 线程池的基本组成部分 线程池管理器（ThreadPool）：用于创建并管理线程池，包括 创建线程池，销毁线程池，添加新任务； 工作线程（WorkThread）：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务； 任务接口（Task）：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等； 任务队列（taskQueue）：用于存放没有处理的任务。提供一种缓冲机制。 线程池状态 ThreadPoolExecutor使用一个变量标记线程池的状态 1private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); 它同时包含两部分的信息：线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)，高3位保存runState，低29位保存workerCount，两个变量之间互不干扰。 用一个变量去存储两个值，可避免在做相关决策时，出现不一致的情况，不必为了维护两者的一致，而占用锁资源。 关于内部封装的获取生命周期状态、获取线程池线程数量的计算方法如以下代码所示： 123private static int runStateOf(int c) { return c &amp; ~CAPACITY; } //计算当前运行状态private static int workerCountOf(int c) { return c &amp; CAPACITY; } //计算当前线程数量private static int ctlOf(int rs, int wc) { return rs | wc; } //通过状态和线程数生成ctl 线程池的运行状态及其变化 线程池任务调度 相关参数 corePoolSize 线程池中的核心线程数，当提交一个任务时，线程池创建一个新线程执行任务，直到当前线程数等于corePoolSize；如果当前线程数为corePoolSize，继续提交的任务被保存到阻塞队列中，等待被执行；如果执行了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有核心线程。 maximumPoolSize 线程池中允许的最大线程数。如果当前阻塞队列满了，且继续提交任务，则创建新的线程执行任务，前提是当前线程数小于maximumPoolSize； keepAliveTim 线程池维护线程所允许的空闲时间。当线程池中的线程数量大于corePoolSize的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了keepAliveTime； 具体实现 首先，所有任务的调度都是由execute方法完成的，这部分完成的工作是：检查现在线程池的运行状态、运行线程数、运行策略，决定接下来执行的流程，是直接申请线程执行，或是缓冲到队列中执行，亦或是直接拒绝该任务。其执行过程如下： 首先检测线程池运行状态，如果不是RUNNING状态，则直接拒绝，线程池要保证在RUNNING的状态下执行任务。 如果workerCount &lt; corePoolSize，则创建并启动一个线程来执行新提交的任务。 如果workerCount &gt;= corePoolSize，且线程池内的阻塞队列未满，则将任务添加到该阻塞队列中。 如果workerCount &gt;= corePoolSize &amp;&amp; workerCount &lt; maximumPoolSize，且线程池内的阻塞队列已满，则创建并启动一个线程来执行新提交的任务。 如果workerCount &gt;= maximumPoolSize，并且线程池内的阻塞队列已满, 则根据拒绝策略来处理该任务, 默认的处理方式是直接抛异常。 其执行流程如下图所示： 任务缓冲与阻塞队列 任务缓冲模块是线程池能够管理任务的核心部分。线程池的本质是对任务和线程的管理，而做到这一点最关键的思想就是将任务和线程两者解耦，不让两者直接关联，才可以做后续的分配工作。线程池中是以生产者消费者模式，通过一个阻塞队列来实现的。阻塞队列缓存任务，工作线程从阻塞队列中获取任务。 阻塞队列 BlockingQueue 通常用于一个线程生产对象，而另外一个线程消费这些对象的场景。下图是对这个原理的阐述: 一个线程往里边放，另外一个线程从里边取的一个 BlockingQueue。 一个线程将会持续生产新对象并将其插入到队列之中，直到队列达到它所能容纳的临界点。也就是说，它是有限的。如果该阻塞队列到达了其临界点，负责生产的线程将会在往里边插入新对象时发生阻塞。它会一直处于阻塞之中，直到负责消费的线程从队列中拿走一个对象。 负责消费的线程将会一直从该阻塞队列中拿出对象。如果消费线程尝试去从一个空的队列中提取对象的话，这个消费线程将会处于阻塞之中，直到一个生产线程把一个对象丢进队列。 线程池中使用不同的阻塞队列可以实现不一样的任务存取策略。 任务申请 在任务提交后，任务的执行会有两种可能的执行方式： 任务直接由新创建的线程执行 线程从任务队列中获取任务然后执行，执行完任务的空闲线程会再次去从队列中申请任务再去执行。 线程需要从任务缓存模块中不断地取任务执行，帮助线程从阻塞队列中获取任务，实现线程管理模块和任务管理模块之间的通信。 getTask这部分进行了多次判断，为的是控制线程的数量，使其符合线程池的状态。如果线程池现在不应该持有那么多线程，则会返回null值。工作线程Worker会不断接收新任务去执行，而当工作线程Worker接收不到任务的时候，就会开始被回收。 任务拒绝和拒绝策略 任务拒绝模块是线程池的保护部分，线程池有一个最大的容量，当线程池的任务缓存队列已满，并且线程池中的线程数目达到maximumPoolSize时，就需要拒绝掉该任务，采取任务拒绝策略，保护线程池。 拒绝策略有一个接口，用户可以定制拒绝策略 123public interface RejectedExecutionHandler { void rejectedExecution(Runnable r, ThreadPoolExecutor executor);} java提供的拒绝策略 Worker线程管理 线程池内的worker，实现了Runnable，并持有一个Thread线程，一个初始化的任务firstTask。通过worker来对线程进行管理。 Worker执行任务的模型如下图所示： Worker是通过继承AQS，实现不可重入的独占锁。 lock方法一旦获取了独占锁，表示当前线程正在执行任务中。 如果正在执行任务，则不应该中断线程。 如果该线程现在不是独占锁的状态，也就是空闲的状态，说明它没有在处理任务，这时可以对该线程进行中断。 线程池在执行shutdown方法或tryTerminate方法时会调用interruptIdleWorkers方法来中断空闲的线程，interruptIdleWorkers方法会使用tryLock方法来判断线程池中的线程是否是空闲状态；如果线程是空闲状态则可以安全回收。 在线程回收过程中就使用到了这种特性，回收过程如下图所示： Worker线程的增加 增加线程是通过线程池中的addWorker方法，该方法的功能就是增加一个线程，该方法不考虑线程池是在哪个阶段增加的该线程，这个分配线程的策略是在上个步骤完成的，该步骤仅仅完成增加线程，并使它运行，最后返回是否成功这个结果。 Worker线程回收 线程池中线程的销毁依赖JVM自动的回收，线程池做的工作是根据当前线程池的状态维护一定数量的线程引用，防止这部分线程被JVM回收，当线程池决定哪些线程需要回收时，只需要将其引用消除即可。 Worker被创建出来后，就会不断地进行轮询，然后获取任务去执行，核心线程可以无限等待获取任务，非核心线程要限时获取任务。当Worker无法获取到任务，也就是获取的任务为空时，循环会结束，Worker会主动消除自身在线程池内的引用。 Worker执行任务 在Worker类中的run方法调用了runWorker方法来执行任务，runWorker方法的执行过程如下： while循环不断地通过getTask()方法获取任务。 getTask()方法从阻塞队列中取任务。 如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态。 执行任务。 如果getTask结果为null则跳出循环，执行processWorkerExit()方法，销毁线程。 不同类型的线程池 在使用不同的阻塞队列和线程池参数，可以将线程池定义为某些特定性质或类型的线程池，使用ThreadPoolExecutor内部的静态方法进行创建 newFixedThreadPool（固定大小线程池） newFixedThreadPool：创建一个核心线程个数和最大线程个数都为 nThreads 的线程池，并且阻塞队列长度为 Integer.MAX_VALUE，keeyAliveTime=0 说明只要线程个数比核心线程个数多并且当前空闲则回收。代码如下： 123456789101112public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); } //使用自定义线程创建工厂 public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory); } newSingleThreadExecutor(单个后台线程） newSingleThreadExecutor：创建一个核心线程个数和最大线程个数都为1的线程池，并且阻塞队列长度为 Integer.MAX_VALUE，keeyAliveTime=0 说明只要线程个数比核心线程个数多并且当前空闲则回收。代码如下： 123456789101112131415public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); }//使用自己的线程工厂public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory));} newCachedThreadPool（无界线程池，可以进行自动线程回收） newCachedThreadPool：创建一个按需创建线程的线程池，初始线程个数为 0，最多线程个数为 Integer.MAX_VALUE，并且阻塞队列为同步队列，keeyAliveTime=60 说明只要当前线程 60s 内空闲则回收。这个特殊在于加入到同步队列的任务会被马上被执行，同步队列里面最多只有一个任务。代码如下： 12345678910111213public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());}//使用自定义的线程工厂public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), threadFactory);} newScheduledThreadPool (可调度) newScheduledThreadPool：创建一个定长线程池，支持定时及周期性任务执行。newScheduledThreadPool 和 其他线程池最大的区别是使用的阻塞队列是 DelayedWorkQueue，而且多了两个定时执行的方法scheduleAtFixedRate和scheduleWithFixedDelay 123456789public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize);}//使用自定义的线程工厂public static ScheduledExecutorService newScheduledThreadPool( int corePoolSize, ThreadFactory threadFactory) { return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory);} newWorkStealingPool（并行操作） newWorkStealingPool：JDK1.8新增newWorkStealingPool，适合使用在很耗时的操作，但是newWorkStealingPool不是ThreadPoolExecutor的扩展，它是新的线程池类ForkJoinPool的扩展，但是都是在统一的一个Executors类中实现，由于能够合理的使用CPU进行对任务操作（并行操作），所以适合使用在很耗时的任务中。代码如下： 12345678910111213public static ExecutorService newWorkStealingPool() { return new ForkJoinPool (Runtime.getRuntime().availableProcessors(), ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, true);}public static ExecutorService newWorkStealingPool(int parallelism) { return new ForkJoinPool (parallelism, ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, true);} Java线程池的使用 Java中创建线程的三种方式 1、继承Thread类创建线程类 123456789public class FirstThreadTest extends Thread { int i = 0; //重写run方法，run方法的方法体就是现场执行体 public void run() { for (; i &lt; 100; i++) { System.out.println(getName() + &quot; &quot; + i); } }} 2、通过Runnable接口创建线程类 12345678910111213public class RunnableThreadTest implements Runnable { private int i; public void run() { for (i = 0; i &lt; 100; i++) { System.out.println(Thread.currentThread().getName() + &quot; &quot; + i); } } public static void main(String[] args) { new Thread(rtt, &quot;新线程2&quot;).start(); }} 3、通过Callable和Future创建线程 创建Callable接口的实现类，并实现call()方法，该call()方法将作为线程执行体，并且有返回值。 123public interface Callable { V call() throws Exception;} 创建Callable实现类的实例，使用FutureTask类来包装Callable对象，该FutureTask对象封装了该Callable对象的call()方法的返回值。（FutureTask是一个包装器，它通过接受Callable来创建，它同时实现了Future和Runnable接口。） 使用FutureTask对象作为Thread对象的target创建并启动新线程。 调用FutureTask对象的get()方法来获得子线程执行结束后的返回值。 123456789Callable&lt;Integer&gt; callable = new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { return null; } }; FutureTask&lt;Integer&gt; integerFutureTask = new FutureTask&lt;&gt;(callable); new Thread(integerFutureTask).start(); integerFutureTask.get(); 这种离散的线程创建方式会导致服务由于资源不足而导致宕机。 线程池创建方式 示例： 123456789101112131415161718192021222324252627282930public class Main { public static void main(String[] args) { // 创建一个固定大小的线程池: ExecutorService es = Executors.newFixedThreadPool(4);//申请一个线程池 for (int i = 0; i &lt; 6; i++) { es.submit(new Task(&quot;&quot; + i)); } // 关闭线程池: es.shutdown(); }}class Task implements Runnable { private final String name; public Task(String name) { this.name = name; } @Override public void run() { System.out.println(&quot;start task &quot; + name); try { Thread.sleep(1000); } catch (InterruptedException e) { } System.out.println(&quot;end task &quot; + name); }} Java通过Executors提供四种线程池 newFixedThreadPool：创建一个固定大小的线程池，提交一个任务就在线程池中创建一个线程，直到线程数量达到线程池最大限制。线程池一旦到最大就不会再改变，除非有已经开启的线程出现异常，再提交任务时会继续新建线程。这个创建线程池的方法是在实际项目中比较常用的，由于corePoolSize和maximumPoolSize两个参数是相同大小，所以到了线程池最大容量后，如果有任务完成让出占用线程，那么此线程就会一直处于等待状态，而不会消亡，直到下一个任务再次占用该线程。此方法的弊端是：使用无界队列来存放排队任务，当大量任务超过线程池最大容量需要处理时，队列无线增大，使服务器资源迅速耗尽。 newCachedThreadPool：创建一个可根据实际情况调整大小的线程池，线程数量不确定，只要有空闲线程空闲时间超过keepAliveTime，就会干掉，再来新任务，先使用空闲线程，若不够，再新建线程。线程池没有最大线程数量限制，所以当大量线程蜂拥而至，会造成资源耗尽。 newSingleThreadExecutor：创建一个容量为1的线程池，被提交任务按优先级依次执行。但是任务队列是无界的LinkedBlockingQueue，存在任务队列无限添加造成OOM的风险。 newScheduledThreadPool：创建一个定长线程池，长度为输入参数自定义，支持定时周期任务执行，可根据时间需要在指定时间对线程进行调度。这个线程池的最大线程数也是Integer.MAX_VALUE，可以理解为会无限创建线程。存在将资源耗尽的风险。 不建议使用 Executors来创建线程池！！！请使用ThreadPoolExecutor的方式创建 线程池应用场景 高并发、任务执行时间短的业务怎样使用线程池？ 线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 并发不高、任务执行时间长的业务怎样使用线程池？ 这个需要判断执行时间是耗在哪个地方 假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以适当加大线程池中的线程数目（2 * CPU核数），让CPU处理更多的业务。 假如是业务时间长集中在计算操作上，也就是CPU密集型任务，和（1）CPU核数+1 一样，线程池中的线程数设置得少一些，减少线程上下文的切换 并发高、业务执行时间长的业务怎样使用线程池？ 解决这种类型任务的关键不在于线程池而在于整体架构的设计 参考文档 https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html https://www.pdai.tech/md/java/thread/java-thread-x-juc-collection-BlockingQueue.html https://juejin.cn/post/6903548111344369672 https://juejin.cn/post/6844903634975768590","link":"cn/java%E5%B9%B6%E5%8F%91(7)/"},{"title":"ForkJoin框架","text":"简介 ForkJoin框架，主要是为了解决并发处理可分割任务的问题，是一个将大任务拆成小任务来异步执行的工具。 基本构成 ForkJoin框架由以下几个模块构成 任务对象：ForkJoinTask (包括RecursiveTask、RecursiveAction 和 CountedCompleter) 执行线程：ForkJoinWorkerThread 线程池：ForkJoinPool 由上图可知，ForkJoinPool属于线程池实现的一种，是ExecutorService接口实现的一种。 ForkJoinWorkerThread类继承自Thread，实现Runnable接口，为ForkJoinPool提供工作线程。 这三者的关系是: ForkJoinPool可以通过池中的ForkJoinWorkerThread来处理ForkJoinTask任务。 使用场景 适合场景 Fork/Join框架适合能够进行拆分再合并的计算密集型（CPU密集型）任务。 ForkJoin框架是一个并行框架，因此要求服务器拥有多CPU、多核，用以提高计算能力。 不适合场景 如果是单核、单CPU，不建议使用该框架，会带来额外的性能开销，反而比单线程的执行效率低。 当然不是因为并行的任务会进行频繁的线程切换，因为Fork/Join框架在进行线程池初始化的时候默认线程数量为Runtime.getRuntime().availableProcessors()，单CPU单核的情况下只会产生一个线程，并不会造成线程切换，而是会增加Fork/Join框架的一些队列、池化的开销。 使用ForkJoin框架处理循环引用的任务会导致无限循环和死锁。 ForkJoin框架的特性 ForkJoinPool 不是为了替代 ExecutorService，而是它的补充，在某些应用场景下性能比 ExecutorService 更好。ForkJoinPool 主要用于实现“分而治之”的算法，特别是分治之后递归调用的函数，例如 quick sort 等； ForkJoinPool 最适合的是计算密集型的任务，如果存在 I/O、线程间同步、sleep() 等会造成线程长时间阻塞的情况时，最好配合 MangedBlocker。 核心思想：分治算法 分治算法，把任务递归的拆分为各个子任务来进行问题的并行处理。提高多核CPU的利用率。 工作窃取算法 工作窃取（work-stealing）算法 是指某个线程从其他队列里窃取任务来执行。 ForkJoinPool中，每一个工作线ForkJoinWorkerThread都对应一个工作队列WorkQueue，工作线程优先以FIFO顺序或者LIFO顺序处理自身队列中的任务，在任务完成后，去其他工作线程的任务队列中取任务执行。 具体设计 为了减少窃取任务线程和被窃取任务线程之间的竞争，会使用双端队列，被窃取任务永远从双端队列的头部拿任务，而窃取任务的线程永远从双端队列的尾部那任务执行。 每个线程都有自己的一个WorkQueue，该工作队列是一个双端队列。 队列支持三个功能push、pop、poll push/pop只能被队列的所有者线程调用，而poll可以被其他线程调用。 划分的子任务调用fork时，都会被push到自己的队列中。 默认情况下，工作线程从自己的双端队列获出任务并执行。 当自己的队列为空时，线程随机从另一个线程的队列末尾调用poll方法窃取任务。 ForkJoin框架原理 ForkJoin框架执行流程 上图可以看出ForkJoinPool 中的任务执行分两种: 直接通过 FJP 提交的外部任务(external/submissions task)，存放在 workQueues 的偶数槽位； 通过内部 fork 分割的子任务(Worker task)，存放在 workQueues 的奇数槽位。 ForkJoinPool ForkJoinPool继承关系如图所示 ForkJoinTask 需要通过 ForkJoinPool 来执行，任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进入队列的头部。当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务。 ForkJoinPool 与 内部类 WorkQueue 共享的一些常量 123456789101112131415161718// Constants shared across ForkJoinPool and WorkQueue// 限定参数static final int SMASK = 0xffff; // 低位掩码，也是最大索引位static final int MAX_CAP = 0x7fff; // 工作线程最大容量static final int EVENMASK = 0xfffe; // 偶数低位掩码static final int SQMASK = 0x007e; // workQueues 数组最多64个槽位// ctl 子域和 WorkQueue.scanState 的掩码和标志位static final int SCANNING = 1; // 标记是否正在运行任务static final int INACTIVE = 1 &lt;&lt; 31; // 失活状态 负数static final int SS_SEQ = 1 &lt;&lt; 16; // 版本戳，防止ABA问题// ForkJoinPool.config 和 WorkQueue.config 的配置信息标记static final int MODE_MASK = 0xffff &lt;&lt; 16; // 模式掩码static final int LIFO_QUEUE = 0; // LIFO队列static final int FIFO_QUEUE = 1 &lt;&lt; 16; // FIFO队列static final int SHARED_QUEUE = 1 &lt;&lt; 31; // 共享模式队列，负数 ForkJoinPool 中的相关常量和实例字段: ForkJoinPool的一些状态字段 12345678910// 实例字段volatile long ctl; // 主控制参数volatile int runState; // 运行状态锁final int config; // 并行度|模式int indexSeed; // 用于生成工作线程索引volatile WorkQueue[] workQueues; // 主对象注册信息，workQueuefinal ForkJoinWorkerThreadFactory factory;// 线程工厂final UncaughtExceptionHandler ueh; // 每个工作线程的异常信息final String workerNamePrefix; // 用于创建工作线程的名称volatile AtomicLong stealCounter; // 偷取任务总数，也可作为同步监视器 ForkJoinPool 的内部状态都是通过一个64位的 long 型 变量ctl来存储，它由四个16位的子域组成: AC: 正在运行工作线程数减去目标并行度，高16位 TC: 总工作线程数减去目标并行度，中高16位 SS: 栈顶等待线程的版本计数和状态，中低16位 ID: 栈顶 WorkQueue 在池中的索引(poolIndex)，低16位 ForkJoinPool的内部数据结构 ForkJoinPool采用了哈希数组 + 双端队列的方式存放任务，但这里的任务分为两类： 一类是通过execute、submit 提交的外部任务 另一类是ForkJoinWorkerThread工作线程通过fork/join分解出来的工作任务 ForkJoinPool并没有把这两种任务混在一个任务队列中，对于外部任务，会利用Thread内部的随机probe值映射到哈希数组的偶数槽位中的提交队列中，这种提交队列是一种数组实现的双端队列称之为Submission Queue，专门存放外部提交的任务。 对于ForkJoinWorkerThread工作线程，每一个工作线程都分配了一个工作队列，这也是一个双端队列，称之为Work Queue，这种队列都会被映射到哈希数组的奇数槽位，每一个工作线程fork/join分解的任务都会被添加到自己拥有的那个工作队列中。 这些队列都储存在ForkJoinPool中的workQueue变量中 1volatile WorkQueue[] workQueues; 如下图所示： 如图，提交队列位于哈希数组workQueue的奇数索引槽位，工作线程的工作队列位于偶数槽位。 构造函数 1234567891011public ForkJoinPool(int parallelism, ForkJoinWorkerThreadFactory factory, UncaughtExceptionHandler handler, boolean asyncMode) { this(checkParallelism(parallelism), checkFactory(factory), handler, asyncMode ? FIFO_QUEUE : LIFO_QUEUE, &quot;ForkJoinPool-&quot; + nextPoolId() + &quot;-worker-&quot;); checkPermission();} 在 ForkJoinPool 中我们可以自定义四个参数: parallelism: 并行度，默认为CPU数，最小为1 factory: 工作线程工厂； handler: 处理工作线程运行任务时的异常情况类，默认为null； asyncMode: 是否为异步模式，默认为 false。如果为true，表示子任务的执行遵循 FIFO 顺序并且任务不能被合并(join)，这种模式适用于工作线程只运行事件类型的异步任务。 在多数场景使用时，如果没有太强的业务需求，我们一般直接使用 ForkJoinPool 中的common池，在JDK1.8之后提供了ForkJoinPool.commonPool()方法可以直接使用common池。 向ForkJoinPool提交任务 向 ForkJoinPool 提交任务有三种方式: invoke()会等待任务计算完毕并返回计算结果； execute()是直接向池提交一个任务来异步执行，无返回结果； submit()也是异步执行，但是会返回提交的任务，在适当的时候可通过task.get()获取执行结果。 这三种提交方式都都是调用externalPush()方法来完成。三种方法都调用了externalPush(ForkJoinTask&lt;?&gt; task)函数 externalPush(ForkJoinTask&lt;?&gt; task) 123456789101112131415161718192021222324252627//添加给定任务到submission队列中final void externalPush(ForkJoinTask&lt;?&gt; task) { WorkQueue[] ws; WorkQueue q; int m; int r = ThreadLocalRandom.getProbe();//探针值，用于计算WorkQueue槽位索引 int rs = runState; if ((ws = workQueues) != null &amp;&amp; (m = (ws.length - 1)) &gt;= 0 &amp;&amp; (q = ws[m &amp; r &amp; SQMASK]) != null &amp;&amp; r != 0 &amp;&amp; rs &gt; 0 &amp;&amp; //获取随机偶数槽位的workQueue U.compareAndSwapInt(q, QLOCK, 0, 1)) {//锁定workQueue ForkJoinTask&lt;?&gt;[] a; int am, n, s; if ((a = q.array) != null &amp;&amp; (am = a.length - 1) &gt; (n = (s = q.top) - q.base)) { int j = ((am &amp; s) &lt;&lt; ASHIFT) + ABASE;//计算任务索引位置 U.putOrderedObject(a, j, task);//任务入列 U.putOrderedInt(q, QTOP, s + 1);//更新push slot U.putIntVolatile(q, QLOCK, 0);//解除锁定 if (n &lt;= 1) signalWork(ws, q);//任务数小于1时尝试创建或激活一个工作线程 return; } U.compareAndSwapInt(q, QLOCK, 1, 0);//解除锁定 } externalSubmit(task);//初始化workQueues及相关属性} externalPush的执行流程如下 首先找到一个随机偶数槽位的 workQueue，然后把任务放入这个 workQueue 的任务数组中，并更新top位。如果队列的剩余任务数小于1，则尝试创建或激活一个工作线程来运行任务(防止在externalSubmit初始化时发生异常导致工作线程创建失败)。 externalSubmit(ForkJoinTask&lt;?&gt; task) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283//任务提交private void externalSubmit(ForkJoinTask&lt;?&gt; task) { //初始化调用线程的探针值，用于计算WorkQueue索引 int r; // initialize caller's probe if ((r = ThreadLocalRandom.getProbe()) == 0) { ThreadLocalRandom.localInit(); r = ThreadLocalRandom.getProbe(); } for (;;) { WorkQueue[] ws; WorkQueue q; int rs, m, k; boolean move = false; if ((rs = runState) &lt; 0) {// 池已关闭 tryTerminate(false, false); // help terminate throw new RejectedExecutionException(); } //初始化workQueues else if ((rs &amp; STARTED) == 0 || // initialize ((ws = workQueues) == null || (m = ws.length - 1) &lt; 0)) { int ns = 0; rs = lockRunState();//锁定runState try { //初始化 if ((rs &amp; STARTED) == 0) { //初始化stealCounter U.compareAndSwapObject(this, STEALCOUNTER, null, new AtomicLong()); //创建workQueues，容量为2的幂次方 // create workQueues array with size a power of two int p = config &amp; SMASK; // ensure at least 2 slots int n = (p &gt; 1) ? p - 1 : 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; n = (n + 1) &lt;&lt; 1; workQueues = new WorkQueue[n]; ns = STARTED; } } finally { unlockRunState(rs, (rs &amp; ~RSLOCK) | ns);//解锁并更新runState } } else if ((q = ws[k = r &amp; m &amp; SQMASK]) != null) {//获取随机偶数槽位的workQueue if (q.qlock == 0 &amp;&amp; U.compareAndSwapInt(q, QLOCK, 0, 1)) {//锁定 workQueue ForkJoinTask&lt;?&gt;[] a = q.array;//当前workQueue的全部任务 int s = q.top; boolean submitted = false; // initial submission or resizing try { // locked version of push if ((a != null &amp;&amp; a.length &gt; s + 1 - q.base) || (a = q.growArray()) != null) {//扩容 int j = (((a.length - 1) &amp; s) &lt;&lt; ASHIFT) + ABASE; U.putOrderedObject(a, j, task);//放入给定任务 U.putOrderedInt(q, QTOP, s + 1);//修改push slot submitted = true; } } finally { U.compareAndSwapInt(q, QLOCK, 1, 0);//解除锁定 } if (submitted) {//任务提交成功，创建或激活工作线程 signalWork(ws, q);//创建或激活一个工作线程来运行任务 return; } } move = true; // move on failure 操作失败，重新获取探针值 } else if (((rs = runState) &amp; RSLOCK) == 0) { // create new queue q = new WorkQueue(this, null); q.hint = r; q.config = k | SHARED_QUEUE; q.scanState = INACTIVE; rs = lockRunState(); // publish index if (rs &gt; 0 &amp;&amp; (ws = workQueues) != null &amp;&amp; k &lt; ws.length &amp;&amp; ws[k] == null) ws[k] = q; // 更新索引k位值的workQueue //else terminated unlockRunState(rs, rs &amp; ~RSLOCK); } else move = true; // move if busy if (move) r = ThreadLocalRandom.advanceProbe(r);//重新获取线程探针值 }} externalSubmit是externalPush的完整版本，主要用于第一次提交任务时初始化workQueues及相关属性，并且提交给定任务到队列中。具体执行步骤如下: 如果池为终止状态(runState&lt;0)，调用tryTerminate来终止线程池，并抛出任务拒绝异常； 如果尚未初始化，就为 FJP 执行初始化操作: 初始化stealCounter、创建workerQueues，然后继续自旋； 初始化完成后，执行在externalPush中相同的操作: 获取 workQueue，放入指定任务。任务提交成功后调用signalWork方法创建或激活线程； 如果在步骤3中获取到的 workQueue 为null，会在这一步中创建一个 workQueue，创建成功继续自旋执行第三步操作； 如果非上述情况，或者有线程争用资源导致获取锁失败，就重新获取线程探针值继续自旋。 ForkJoinPool.execute方法 12345public void execute(ForkJoinTask&lt;?&gt; task) { if (task == null) throw new NullPointerException(); externalPush(task);} ForkJoinPool.submit 方法 1234567public &lt;T&gt; ForkJoinTask&lt;T&gt; submit(ForkJoinTask&lt;T&gt; task) { if (task == null) throw new NullPointerException(); //提交到工作队列 externalPush(task); return task;} ForkJoinPool 自身拥有工作队列，这些工作队列的作用是用来接收由外部线程（非 ForkJoinThread 线程）提交过来的任务，而这些工作队列被称为 submitting queue 。 submit() execute() 和 fork() 其实没有本质区别，只是提交对象变成了 submitting queue 而已（还有一些同步，初始化的操作）。submitting queue 和其他 work queue 一样，是工作线程”窃取“的对象，因此当其中的任务被一个工作线程成功窃取时，就意味着提交的任务真正开始进入执行阶段。 ForkJoinWorkerThread 这个类是ForkJoinPool的任务执行线程类，继承自Thread。该类需要在声明的时候确定其所在的线程池，同时在线程类内部定义一个工作队列用于任务的记录。 12345678910111213141516171819202122232425262728293031323334353637383940public class ForkJoinWorkerThread extends Thread { // 这个线程工作的ForkJoinPool池 final ForkJoinPool pool; // 这个线程拥有的工作窃取机制的工作队列 final ForkJoinPool.WorkQueue workQueue; //创建在给定ForkJoinPool池中执行的ForkJoinWorkerThread。 protected ForkJoinWorkerThread(ForkJoinPool pool) { // Use a placeholder until a useful name can be set in registerWorker super(&quot;aForkJoinWorkerThread&quot;); this.pool = pool; //向ForkJoinPool执行池注册当前工作线程，ForkJoinPool为其分配一个工作队列 this.workQueue = pool.registerWorker(this); } //该工作线程的执行内容就是执行工作队列中的任务 public void run() { if (workQueue.array == null) { // only run once Throwable exception = null; try { onStart(); pool.runWorker(workQueue); //执行工作队列中的任务 } catch (Throwable ex) { exception = ex; //记录异常 } finally { try { onTermination(exception); } catch (Throwable ex) { if (exception == null) exception = ex; } finally { pool.deregisterWorker(this, exception); //撤销工作 } } } } .....} ForkJoinTask ForkJoinTask类是用来定义ForkJoinPool线程池中需要执行的任务。与FutureTask一样， ForkJoinTask也是Future的子类，不过它是一个抽象类。 要使用 ForkJoin 框架，必须首先创建一个 ForkJoin 任务。它提供在任务中执行 fork() 和 join() 操作的机制，通常情况下我们不需要直接继承 ForkJoinTask 类，而只需要继承它的子类，Fork/Join框架提供类以下几个子类： RecursiveAction：用于没有返回结果的任务。 RescursiveTask：用于有返回结果的任务。 CountedCompleter ：在任务完成执行后会触发执行一个自定义的钩子函数。 ForkJoinTask由一个int类型的Status字段 其高16位存储任务执行状态例如NORMAL、CANCELLED或EXCEPTIONAL 低16位预留用于用户自定义的标记。 1volatile int status; // accessed directly by pool and workers 除了status记录任务的执行状态外，还需要对任务的异常进行处理，ForkJoinTask采用了哈希数组 + 链表的数据结构（JDK8以前的HashMap实现方法）存放所有（因为这些字段是static）的ForkJoinTask任务的执行异常。 Fork方法 fork()函数的作用是将任务通过push方法加入当前工作线程的工作队列，若当前线程不是ForkJoinWorkerThread,则会被提交到ForkJoinPool中的提交队列中。 12345678public final ForkJoinTask&lt;V&gt; fork() { Thread t; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ((ForkJoinWorkerThread)t).workQueue.push(this); else ForkJoinPool.common.externalPush(this); return this;} ForkJoinPool线程池通过哈希数组+双端队列的方式将所有的工作线程拥有的任务队列和从外部提交的任务分别映射到哈希数组的不同槽位上。 Join方法 join()函数，在fork调用之后使用，返回函数执行结果。 执行过程如下： 检查调用 join() 的线程是否是 ForkJoinThread 线程。如果不是（例如 main 线程），则阻塞当前线程，等待任务完成。如果是，则不阻塞。 查看任务的完成状态，如果已经完成，直接返回结果。 如果任务尚未完成，但处于自己的工作队列内，则完成它。 如果任务已经被其他的工作线程偷走，则窃取这个小偷的工作队列内的任务（以 FIFO 方式），执行，以期帮助它早日完成欲 join 的任务。 如果偷走任务的小偷也已经把自己的任务全部做完，正在等待需要 join 的任务时，则找到小偷的小偷，帮助它完成它的任务。 递归地执行第5步。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231//当计算完成时返回计算结果。此方法与get()的不同之处在于//异常完成会导致RuntimeException或Error//而不是ExecutionException//调用线程被中断不会通过抛出InterruptedException导致方法突然返回。public final V join() { int s; if ((s = doJoin() &amp; DONE_MASK) != NORMAL) reportException(s); //非正常结束，抛出相关的异常堆栈信息 return getRawResult(); //正常结束，返回结果}//等待任务执行结束并返回其状态status，该方法实现了join, get, quietlyJoin. //直接处理已经完成的，外部等待和unfork+exec的情况//其它情况转发到ForkJoinPool.awaitJoin//如果 status &lt; 0 则返回s；//否则，若不是ForkJoinWorkerThread ，则等待 externalAwaitDone() 返回//否则，若 (w = (wt = (ForkJoinWorkerThread)t).workQueue).tryUnpush(this) &amp;&amp; (s = doExec()) &lt; 0 则 返回s；//否则，返回 wt.pool.awaitJoin(w, this, 0L)private int doJoin() { int s; Thread t; ForkJoinWorkerThread wt; ForkJoinPool.WorkQueue w; return (s = status) &lt; 0 ? s : //status为负数表示任务已经执行结束，直接返回status。 ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ? (w = (wt = (ForkJoinWorkerThread)t).workQueue). tryUnpush(this) &amp;&amp; (s = doExec()) &lt; 0 ? s : //调用pool的执行逻辑，并等待返回执行结果状态 wt.pool.awaitJoin(w, this, 0L) : //调用pool的等待机制 externalAwaitDone(); //不是ForkJoinWorkerThread，}//抛出与给定状态关联的异常(如果有),被取消是CancellationException。private void reportException(int s) { if (s == CANCELLED) throw new CancellationException(); if (s == EXCEPTIONAL) rethrow(getThrowableException());}public abstract V getRawResult();//返回给定任务的执行异常（如果有的话），为了提供准确的异常堆栈信息，//若异常不是由当前线程抛出的//将尝试以记录的异常为原因创建一个与抛出异常类型相同的新异常。//如果没有那样的构造方法将尝试使用无参的构造函数//并通过设置initCause方法以达到同样的效果，尽管它可能包含误导的堆栈跟踪信息。private Throwable getThrowableException() { if ((status &amp; DONE_MASK) != EXCEPTIONAL) return null; //1. 通过当前任务对象的哈希值到哈希链表数组中找到相应的异常节点 int h = System.identityHashCode(this); //当前任务的hash值 ExceptionNode e; final ReentrantLock lock = exceptionTableLock; lock.lock(); //加锁 try { expungeStaleExceptions(); //清理被GC回收的任务的异常节点 ExceptionNode[] t = exceptionTable; e = t[h &amp; (t.length - 1)]; //通过取模对应得索引获取哈希数组槽位中得节点 while (e != null &amp;&amp; e.get() != this) e = e.next; //遍历找到当前任务对应的异常节点 } finally { lock.unlock(); } Throwable ex; if (e == null || (ex = e.ex) == null) //表示没有出现任何异常 return null; if (e.thrower != Thread.currentThread().getId()) { //有异常但是不是由当前线程抛出的 Class&lt;? extends Throwable&gt; ec = ex.getClass(); try { Constructor&lt;?&gt; noArgCtor = null; Constructor&lt;?&gt;[] cs = ec.getConstructors();// public ctors only //通过反射找到构造方法，并构造新异常 for (int i = 0; i &lt; cs.length; ++i) { Constructor&lt;?&gt; c = cs[i]; Class&lt;?&gt;[] ps = c.getParameterTypes(); if (ps.length == 0) noArgCtor = c; //记录下无参构造方法，以备没有找到期望的构造方法时使用 else if (ps.length == 1 &amp;&amp; ps[0] == Throwable.class) { Throwable wx = (Throwable)c.newInstance(ex); //发现了我们期望的Throwable类型的参数的构造方法 return (wx == null) ? ex : wx; } } if (noArgCtor != null) { //没有找到期望的构造方法，只能通过无参构造方法创建新异常 Throwable wx = (Throwable)(noArgCtor.newInstance()); if (wx != null) { wx.initCause(ex); //将原始异常设置进去 return wx; } } } catch (Exception ignore) { } } return ex;}//清除哈希链表数组中已经被GC回收掉的任务的异常节点。//从exceptionTableRefQueue节点引用队列中获取异常节点并移除哈希链表数组中得对应节点private static void expungeStaleExceptions() { for (Object x; (x = exceptionTableRefQueue.poll()) != null;) { if (x instanceof ExceptionNode) { int hashCode = ((ExceptionNode)x).hashCode; //节点hash ExceptionNode[] t = exceptionTable; int i = hashCode &amp; (t.length - 1); //取模得到哈希表索引 ExceptionNode e = t[i]; ExceptionNode pred = null; while (e != null) { ExceptionNode next = e.next; if (e == x) { //找到了目标节点 if (pred == null) t[i] = next; else pred.next = next; break; } pred = e; //往后遍历链表 e = next; } } }}//窃取任务的主要执行方法，除非已经完成了，否则调用exec()并记录完成时的状态。final int doExec() { int s; boolean completed; if ((s = status) &gt;= 0) { //任务还未完成 try { completed = exec(); 调用exec()并记录完成时的状态。 } catch (Throwable rex) { return setExceptionalCompletion(rex); //记录异常并返回相关状态，并唤醒通过join等待此任务的线程。 } if (completed) s = setCompletion(NORMAL); //更新状态为正常结束，并唤醒通过join等待此任务的线程。 } return s;}//立即执行此任务的基本操作。返回true表示该任务已经正常完成//否则返回false表示此任务不一定完成(或不知道是否完成)。//此方法还可能抛出(未捕获的)异常，以指示异常退出。//此方法旨在支持扩展，一般不应以其他方式调用。protected abstract boolean exec();//等待未完成的非ForkJoinWorkerThread线程提交的任务执行结束，并返回任务状态statusprivate int externalAwaitDone() { //若是CountedCompleter任务，等待ForkJoinPool.common.externalHelpComplete((CountedCompleter&lt;?&gt;)this, 0) 返回 //否则，若ForkJoinPool.common.tryExternalUnpush(this)，返回 doExec() 结果； //否则，返回0 int s = ((this instanceof CountedCompleter) ? // try helping ForkJoinPool.common.externalHelpComplete( (CountedCompleter&lt;?&gt;)this, 0) : //辅助完成外部提交的CountedCompleter任务 ForkJoinPool.common.tryExternalUnpush(this) ? doExec() : 0); //辅助完成外部提交的非CountedCompleter任务 if (s &gt;= 0 &amp;&amp; (s = status) &gt;= 0) { //表示任务还没结束，需要阻塞等待。 boolean interrupted = false; do { if (U.compareAndSwapInt(this, STATUS, s, s | SIGNAL)) { //标记有线程需要被唤醒 synchronized (this) { if (status &gt;= 0) { try { wait(0L); //任务还没结束，无限期阻塞直到被唤醒 } catch (InterruptedException ie) { interrupted = true; } } else notifyAll(); //已经结束了唤醒所有阻塞的线程 } } } while ((s = status) &gt;= 0); if (interrupted) Thread.currentThread().interrupt(); //恢复中断标识 } return s;}//记录异常，更新status状态，唤醒所有等待线程private int setExceptionalCompletion(Throwable ex) { int s = recordExceptionalCompletion(ex); if ((s &amp; DONE_MASK) == EXCEPTIONAL) internalPropagateException(ex); //调用钩子函数传播异常 return s;}/** * 对任务异常结束的异常传播支持的钩子函数 */void internalPropagateException(Throwable ex) {}//记录异常并设置状态statusfinal int recordExceptionalCompletion(Throwable ex) { int s; if ((s = status) &gt;= 0) { int h = System.identityHashCode(this); //哈希值 final ReentrantLock lock = exceptionTableLock; lock.lock(); //加锁 try { expungeStaleExceptions(); ExceptionNode[] t = exceptionTable; int i = h &amp; (t.length - 1); for (ExceptionNode e = t[i]; ; e = e.next) { if (e == null) { //遍历完了都没找到，说明哈希链表数组中不存在该任务对于的异常节点 t[i] = new ExceptionNode(this, ex, t[i]); //创建一个异常节点用头插法插入哈希链表数组 break; } if (e.get() == this) // 哈希链表数组中已经存在相应的异常节点，退出 break; } } finally { lock.unlock(); } s = setCompletion(EXCEPTIONAL); } return s;}//标记任务完成标志，并唤醒通过join等待此任务的线程。private int setCompletion(int completion) { for (int s;;) { if ((s = status) &lt; 0) return s; if (U.compareAndSwapInt(this, STATUS, s, s | completion)) { //更新状态 if ((s &gt;&gt;&gt; 16) != 0) synchronized (this) { notifyAll(); } //唤醒所有等待线程 return completion; } }} get 方法（获取异步任务结果） 作为Future的子类，必然要实现Future中的get函数 12345678910public final V get() throws InterruptedException, ExecutionException { int s = (Thread.currentThread() instanceof ForkJoinWorkerThread) ? doJoin() : //是ForkJoinWorkerThread，执行doJoin externalInterruptibleAwaitDone(); //执行externalInterruptibleAwaitDone Throwable ex; if ((s &amp;= DONE_MASK) == CANCELLED) throw new CancellationException(); //被取消的抛出CancellationException if (s == EXCEPTIONAL &amp;&amp; (ex = getThrowableException()) != null) throw new ExecutionException(ex); //执行中出现异常的抛出相应的异常 return getRawResult(); //返回正常结果} ForkJoin框架的异常处理 ForkJoinTask类提供了以下两个方法： 123public final boolean isCompletedAbnormally() { return status &lt; NORMAL;} 以上方法其实就是做个任务状态的判断，如果任务抛出了异常，或者被取消，则返回true。 123456public final Throwable getException() { int s = status &amp; DONE_MASK; return ((s &gt;= NORMAL) ? null : (s == CANCELLED) ? new CancellationException() : getThrowableException());} 以上方法其实也是做任务状态的比较，然后决定是返回null，还是返回异常，该方法可以得到返回值。 通过以上两个方法的结合，就可以做到对ForkJoinTask的异常处理。 例如: 1234567ForkJoinTask task;//ForkJoinTask是一个接口，不能是实例化，这里默认task是ForkJoinTask子类的实例对象//如果该方法返回true，说明任务执行过程抛出了异常或者已经被取消if(task.isCompletedAbnormally){ //返回Throwable对象，如果任务被取消，返回CancellationException异常，如果任务没出现异常或者没完成，则返回null System.out.println(task.getException());} 参考资料 https://pdai.tech/md/java/thread/java-thread-x-juc-executor-ForkJoinPool.html https://juejin.cn/post/6906424424967667725","link":"cn/java%E5%B9%B6%E5%8F%91(8)/"},{"title":"Java设计模式总结","text":"设计模式，是指在软件设计中，被反复使用的一种代码设计经验。使用设计模式的目的是为了可重用代码，提高代码的可扩展性和可维护性。 创建型： 工厂方法：定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method使一个类的实例化延迟到其子类。（静态工厂方法） 抽象工厂：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。 生成器：将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示 单例：确保一个类只有一个实例，并提供该实例的全局访问点 原型：用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象 结构型： 装饰器：动态地将责任附加到对象上, 若要扩展功能, 装饰者提供了比继承更有弹性的替代方案。 适配器：将一个类的接口, 转换成客户期望的另一个接口。 适配器让原本接口不兼容的类可以合作无间。 代理：为另一个对象提供一个替身或占位符以控制对这个对象的访问 行为型： 模板方法：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中，使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤 观察者：定义对象之间的一对多依赖，当一个对象状态改变时，它的所有依赖都会收到通知并且自动更新状态。 责任链：通过责任链模式, 你可以为某个请求创建一个对象链. 每个对象依序检查此请求并对其进行处理或者将它传给链中的下一个对象。 策略：定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。 工厂方法模式 首先是接口： 123456789public interface NumberFactory { Number parse(String s); // 获取工厂实例: static NumberFactory getFactory() { return impl; } static NumberFactory impl = new NumberFactoryImpl(); //在接口Factory中定义一个静态方法getFactory()来返回真正的子类} 以及一个实现类： 12345public class NumberFactoryImpl implements NumberFactory { public Number parse(String s) { return new BigDecimal(s); }} 客户端的效果： 12NumberFactory factory = NumberFactory.getFactory();Number result = factory.parse(&quot;123.456&quot;); 由此杜绝了客户端知晓真正的工程NumberFactoryImpl和产品BigDecimal，实现了封装 通常会使用静态方法直接返回产品，称之为静态工厂方法: 12345public class NumberFactory { public static Number parse(String s) { return new BigDecimal(s); }} java标准库中的静态工厂方法： 1Integer n = Integer.valueOf(100); 抽象工厂模式 而工厂方法模式只是用于创建一个对象，这和抽象工厂模式有很大不同。 抽象工厂模式创建的是对象家族，也就是很多对象而不是一个对象，并且这些对象是相关的，也就是说必须一起创建出来。 123456public class AbstractProductA {}public class AbstractProductB {}public class ProductA1 extends AbstractProductA {}public class ProductA2 extends AbstractProductA {}public class ProductB1 extends AbstractProductB {}public class ProductB2 extends AbstractProductB {} 1234public abstract class AbstractFactory { abstract AbstractProductA createProductA(); abstract AbstractProductB createProductB();} 12345678public class ConcreteFactory1 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA1(); } AbstractProductB createProductB() { return new ProductB1(); }} 12345678public class ConcreteFactory2 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA2(); } AbstractProductB createProductB() { return new ProductB2(); }} 12345678public class Client { public static void main(String[] args) { AbstractFactory abstractFactory = new ConcreteFactory1(); AbstractProductA productA = abstractFactory.createProductA(); AbstractProductB productB = abstractFactory.createProductB(); // do something with productA and productB }} 当需要创建一个对象来进行使用的时候，可能会需要多个相关的对象（功能相似，或者结构相似，属性相似等），则可以封装在一个抽象的工厂类中，由这个抽象的工厂类，调用实际的工厂进行产品的生产。客户只需要声明抽象类，并指定一个具体的工厂，来获得指定的产品，甚至可以根据传入的参数，自动决定工厂来获得产品。 生成器模式 生成器模式（Builder）是使用多个“小型”工厂来最终创建出一个完整对象。 当我们使用Builder的时候，一般来说，是因为创建这个对象的步骤比较多，每个步骤都需要一个零部件，最终组合成一个完整的对象。 通过不断的传入对象对产品进行生成。 典型如链式调用。 单例模式 单例模式（Singleton）的目的是为了保证在一个进程中，某个类有且仅有一个实例。 通过，修饰单例的构造方法为private，并提供一个静态方法，直接返回实例 12345678910111213public class Singleton { // 静态字段引用唯一实例: private static final Singleton INSTANCE = new Singleton(); // 通过静态方法返回实例: public static Singleton getInstance() { return INSTANCE; } // private构造方法保证外部无法实例化: private Singleton() { }} 或者直接把static变量暴露给外部： 12345678public class Singleton { // 静态字段引用唯一实例: public static final Singleton INSTANCE = new Singleton(); // private构造方法保证外部无法实例化: private Singleton() { }} 懒汉式 ——私有静态变量被延迟实例化： 123456789101112public class Singleton { private static Singleton INSTANCE = null; public static Singleton getInstance() { if (INSTANCE == null) { INSTANCE = new Singleton(); } return INSTANCE; } private Singleton() { }} 好处：没有用到的时候可以节省内存空间 坏处：线程不安全，可能多次创建。通过加锁实现线程安全。 饿汉式——私有静态变量直接实例化 1public static final Singleton INSTANCE = new Singleton(); 双重校验锁 于Java的内存模型，双重检查在这里不成立。要真正实现延迟加载，只能通过Java的ClassLoader机制完成 其他——通过枚举实现单例模式 1234567891011121314public enum World { // 唯一枚举: INSTANCE; private String name = &quot;world&quot;; public String getName() { return this.name; } public void setName(String name) { this.name = name; }} 原型 使用原型实例指定要创建对象的类型，通过复制这个原型来创建新对象。 就是拷贝 装饰模式 装饰器（Decorator）模式，是一种在运行期动态给某个对象的实例增加功能的方法 Decorator模式的目的就是把一个一个的附加功能，用Decorator的方式给一层一层地累加到原始数据源上，最终，通过组合获得我们想要的功能。 123456789101112131415 ┌───────────┐ │ Component │ └───────────┘ ▲ ┌────────────┼─────────────────┐ │ │ │┌───────────┐┌───────────┐ ┌───────────┐│ComponentA ││ComponentB │... │ Decorator │└───────────┘└───────────┘ └───────────┘ ▲ ┌──────┴──────┐ │ │ ┌───────────┐ ┌───────────┐ │DecoratorA │ │DecoratorB │... └───────────┘ └───────────┘ 123public interface Beverage { double cost();} 123456public class DarkRoast implements Beverage { @Override public double cost() { return 1; }} 1234567public class HouseBlend implements Beverage { @Override public double cost() { return 1; }} 123public abstract class CondimentDecorator implements Beverage { protected Beverage beverage;} 1234567891011public class Milk extends CondimentDecorator { public Milk(Beverage beverage) { this.beverage = beverage; } @Override public double cost() { return 1 + beverage.cost(); }} 1234567891011public class Mocha extends CondimentDecorator { public Mocha(Beverage beverage) { this.beverage = beverage; } @Override public double cost() { return 1 + beverage.cost(); }} 12345678public class Client { public static void main(String[] args) { Beverage beverage = new HouseBlend(); beverage = new Mocha(beverage); beverage = new Milk(beverage); System.out.println(beverage.cost()); }} Mocha Milk 功能，通过继承CondimentDecorator，传入Beverage指针，实现Beverage功能，同时实现器本身的功能，用户代码需要修改。 代理模式 相当于一个同一个类转化为同一个类Adapter ，但是在其中可以存在源类中没有的一些功能，譬如权限查询，加密解密等。 模板方法 模板方法（Template Method）是一个比较简单的模式。它的主要思想是，定义一个操作的一系列步骤，对于某些暂时确定不下来的步骤，就留给子类去实现好了，这样不同的子类就可以定义出不同的步骤。 模板本身为虚类，实现基础的功能或算法，暂时确定不下来的步骤，作为虚函数，由子类继承 12345678910111213141516171819202122public abstract class CaffeineBeverage { final void prepareRecipe() { boilWater(); brew(); pourInCup(); addCondiments(); } abstract void brew(); abstract void addCondiments(); void boilWater() { System.out.println(&quot;boilWater&quot;); } void pourInCup() { System.out.println(&quot;pourInCup&quot;); }} 由子类实现具体算法来实现多态 123456789101112public class Coffee extends CaffeineBeverage { @Override void brew() { System.out.println(&quot;Coffee.brew&quot;); } @Override void addCondiments() { System.out.println(&quot;Coffee.addCondiments&quot;); }} 123456789101112public class Tea extends CaffeineBeverage { @Override void brew() { System.out.println(&quot;Tea.brew&quot;); } @Override void addCondiments() { System.out.println(&quot;Tea.addCondiments&quot;); }} 123456789public class Client { public static void main(String[] args) { CaffeineBeverage caffeineBeverage = new Coffee(); caffeineBeverage.prepareRecipe(); System.out.println(&quot;-----------&quot;); caffeineBeverage = new Tea(); caffeineBeverage.prepareRecipe(); }} 观察者模式 观察者模式（Observer）又称发布-订阅模式（Publish-Subscribe：Pub/Sub）。它是一种通知机制，让发送通知的一方（被观察方）和接收通知的一方（观察者）能彼此分离，互不影响。 实现方式是建立一个Obsever接口，想要观察的类，需要注册到引用Observer接口的类 12345678910┌─────────┐ ┌───────────────┐│ Store │─ ─ ─&gt;│ProductObserver│└─────────┘ └───────────────┘ │ ▲ │ │ ┌─────┴─────┐ ▼ │ │┌─────────┐ ┌─────────┐ ┌─────────┐│ Product │ │ Admin │ │Customer │ ...└─────────┘ └─────────┘ └─────────┘ 广义的观察者模式包括所有消息系统。所谓消息系统，就是把观察者和被观察者完全分离，通过消息系统本身来通知： 12345678910111213 ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ Messaging System │ │ ┌──────────────────┐ ┌──┼&gt;│Topic:newProduct │──┼─┐ ┌─────────┐ │ └──────────────────┘ ├───&gt;│ConsumerA│┌─────────┐ │ │ ┌──────────────────┐ │ │ └─────────┘│Producer │───┼───&gt;│Topic:priceChanged│────┘└─────────┘ │ │ └──────────────────┘ │ │ ┌──────────────────┐ ┌─────────┐ └──┼&gt;│Topic:soldOut │──┼─────&gt;│ConsumerB│ └──────────────────┘ └─────────┘ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ 消息发送方称为Producer，消息接收方称为Consumer，Producer发送消息的时候，必须选择发送到哪个Topic。Consumer可以订阅自己感兴趣的Topic，从而只获得特定类型的消息。 由于观察者模式，通知由Product对象提出，会阻塞线程，需要采用异步方式实现具体被观察者。 责任链模型 责任链模式（Chain of Responsibility）是一种处理请求的模式，它让多个处理器都有机会处理该请求，直到其中某个处理成功为止。责任链模式把多个处理器串成链，然后让请求在链上传递。 1234567891011121314151617181920212223 ┌─────────┐ │ Request │ └─────────┘ │┌ ─ ─ ─ ─ ┼ ─ ─ ─ ─ ┐ ▼│ ┌─────────────┐ │ │ ProcessorA ││ └─────────────┘ │ ││ ▼ │ ┌─────────────┐│ │ ProcessorB │ │ └─────────────┘│ │ │ ▼│ ┌─────────────┐ │ │ ProcessorC ││ └─────────────┘ │ │└ ─ ─ ─ ─ ┼ ─ ─ ─ ─ ┘ │ ▼ 策略 策略模式：Strategy，是指，定义一组算法，并把其封装到一个对象中。然后在运行时，可以灵活的使用其中的一个算法。 可以是使用传入参数来选择算法，也可以传入一个算法实现类进行算法选择 传入参数选择示例 123456789public class Client { public static void main(String[] args) { Duck duck = new Duck(); duck.setQuackBehavior(new Squeak()); duck.performQuack(); duck.setQuackBehavior(new Quack()); duck.performQuack(); }} 1234567891011121314151617181920public class Main { public static void main(String[] args) throws InterruptedException { String[] array = { &quot;apple&quot;, &quot;Pear&quot;, &quot;Banana&quot;, &quot;orange&quot; }; sort(array, String::compareToIgnoreCase); System.out.println(Arrays.toString(array)); } static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c) { for (int i = 0; i &lt; a.length - 1; i++) { for (int j = 0; j &lt; a.length - 1 - i; j++) { if (c.compare(a[j], a[j + 1]) &gt; 0) { T temp = a[j]; a[j] = a[j + 1]; a[j + 1] = temp; } } } }}","link":"cn/java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"记一次Springboot+mybaits配置经历","text":"2021年，八月，在学习springboot+mybatis的时候遇到了一些问题： spring启动错误以及mybaits绑定失败 刚开始，在配置springboot＋mybaits的时候，根据网络上的博客，配置application.yml，配置UserMapper等文件 application.yml 1234567891011spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/t_blog?useUnicode=true&amp;useSSL=false&amp;characterEncoding=utf8&amp;serverTimezone=Asia/Shanghai username: root password: 123456mybatis: type-aliases-package: com.mybatislearn.mockingjay.entity mapper-locations: classpath:mapper/*.xmlserver: port: 8080 UserMapper.xml 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.mybatislearn.mockingjay.mapper.UserMapper&quot;&gt; &lt;resultMap id=&quot;UserMap&quot; type=&quot;com.mybatislearn.mockingjay.entity.User&quot; &gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;avatar&quot; property=&quot;avatar&quot; /&gt; &lt;result column=&quot;createTime&quot; property=&quot;createTime&quot; /&gt; &lt;result column=&quot;email&quot; property=&quot;email&quot; /&gt; &lt;result column=&quot;nickname&quot; property=&quot;nickname&quot; /&gt; &lt;result column=&quot;password&quot; property=&quot;password&quot; /&gt; &lt;result column=&quot;type&quot; property=&quot;type&quot; /&gt; &lt;result column=&quot;updateTime&quot; property=&quot;updateTime&quot; /&gt; &lt;result column=&quot;username&quot; property=&quot;username&quot; /&gt; &lt;/resultMap&gt; &lt;!-- 根据用户名查询用户--&gt; &lt;select id=&quot;loadUserByUsername&quot; resultMap=&quot;UserMap&quot;&gt; select * from t_user where username=#{username} &lt;/select&gt;&lt;/mapper&gt; 配置一个mapper接口 UserMapper.java 1234567package com.mybatislearn.mockingjay.mapper;import com.mybatislearn.mockingjay.entity.User;public interface UserMapper { User loadUserByUsername(String username);} 同时项目内也是MVC结构的： service UserService.java 123456package com.mybatislearn.mockingjay.service;import com.mybatislearn.mockingjay.entity.User;public interface UserService { User queryUserByname(String name);} UserServiceImpl.java 123456789101112131415161718package com.mybatislearn.mockingjay.service;import com.mybatislearn.mockingjay.entity.User;import com.mybatislearn.mockingjay.mapper.UserMapper;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class UserServiceImpl implements UserService{ @Autowired private UserMapper userMapper; @Override public User queryUserByname(String name) { User user = userMapper.loadUserByUsername(name); return user; }} Controller 123456789101112131415161718192021222324package com.mybatislearn.mockingjay.controller;import com.mybatislearn.mockingjay.entity.User;import com.mybatislearn.mockingjay.mapper.UserMapper;import com.mybatislearn.mockingjay.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class UserController { @Autowired private UserService userService; @GetMapping(&quot;/getuser&quot;) public User GetUser(String name) { System.out.println(name); User user = userService.queryUserByname(name); return user; }} 最后是一个User实体 User.java 12345678910111213141516171819202122232425262728package com.mybatislearn.mockingjay.entity;import lombok.Data;import lombok.ToString;import java.time.LocalDateTime;@Data@ToStringpublic class User { private Long id; private String avatar; private LocalDateTime createTime; private String email; private String nickname; private String password; private Integer type; private LocalDateTime updateTime; private String username;} 最后是pom文件，这里只取maven依赖 1234567891011121314151617181920212223242526272829303132333435363738&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;5.3.8&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在配置完成后出现了springboot启动失败的问题： 打印的日志 12345678910111213141516171819202122232425262728293031APPLICATION FAILED TO STARTDescription:An attempt was made to call a method that does not exist. The attempt was made from the following location: org.springframework.web.servlet.handler.AbstractHandlerMethodMapping.createHandlerMethod(AbstractHandlerMethodMapping.java:341)The following method did not exist: 'void org.springframework.web.method.HandlerMethod.&lt;init&gt;(java.lang.String, org.springframework.beans.factory.BeanFactory, org.springframework.context.MessageSource, java.lang.reflect.Method)'The method's class, org.springframework.web.method.HandlerMethod, is available from the following locations:```jar:file:/C:/Users/xzh11/.m2/repository/org/springframework/spring-web/5.3.8/spring-web-5.3.8.jar!/org/springframework/web/method/HandlerMethod.class```The class hierarchy was loaded from the following locations:```org.springframework.web.method.HandlerMethod: file:/C:/Users/xzh11/.m2/repository/org/springframework/spring-web/5.3.8/spring-web-5.3.8.jar```Action:Correct the classpath of your application so that it contains a single, compatible version of org.springframework.web.method.HandlerMethodProcess finished with exit code 0 在查阅资料后，我觉得是我使用的spring版本的问题，项目内我使用的是2.5.4snapshot 1&lt;version&gt;2.5.4-SNAPSHOT&lt;/version&gt; 进入官网后，看到springboot最新的稳定版本为2.5.3 springboot官网 替换springboot版本为2.5.3 1&lt;version&gt;2.5.3&lt;/version&gt; 成功启动springboot！ mybais报错 Invalid bound statement 在启动springboot后，对Controller进行测试，结果出现mybatis报错Invalid bound statement 1234567891011org.apache.ibatis.binding.BindingException: Invalid bound statement (not found): com.mybatislearn.mockingjay.mapper.UserMapper.loadUserByUsername at org.apache.ibatis.binding.MapperMethod$SqlCommand.&lt;init&gt;(MapperMethod.java:235) ~[mybatis-3.5.6.jar:3.5.6] at org.apache.ibatis.binding.MapperMethod.&lt;init&gt;(MapperMethod.java:53) ~[mybatis-3.5.6.jar:3.5.6] at org.apache.ibatis.binding.MapperProxy.lambda$cachedInvoker$0(MapperProxy.java:115) ~[mybatis-3.5.6.jar:3.5.6] at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1708) ~[na:na] at org.apache.ibatis.binding.MapperProxy.cachedInvoker(MapperProxy.java:102) ~[mybatis-3.5.6.jar:3.5.6] at org.apache.ibatis.binding.MapperProxy.invoke(MapperProxy.java:85) ~[mybatis-3.5.6.jar:3.5.6] at com.sun.proxy.$Proxy57.loadUserByUsername(Unknown Source) ~[na:na] at com.mybatislearn.mockingjay.service.UserServiceImpl.queryUserByname(UserServiceImpl.java:15) ~[classes/:na] at com.mybatislearn.mockingjay.controller.UserController.GetUser(UserController.java:21) ~[classes/:na] ...... 在折腾了一上午之后，发现还是配置的错误： 在UserMapper文件中的namespace没有写对！！！！！ 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.mybatislearn.mockingjay.mapper.UserMapper&quot;&gt; &lt;resultMap id=&quot;UserMap&quot; type=&quot;com.mybatislearn.mockingjay.entity.User&quot; &gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;avatar&quot; property=&quot;avatar&quot; /&gt; &lt;result column=&quot;createTime&quot; property=&quot;createTime&quot; /&gt; &lt;result column=&quot;email&quot; property=&quot;email&quot; /&gt; &lt;result column=&quot;nickname&quot; property=&quot;nickname&quot; /&gt; &lt;result column=&quot;password&quot; property=&quot;password&quot; /&gt; &lt;result column=&quot;type&quot; property=&quot;type&quot; /&gt; &lt;result column=&quot;updateTime&quot; property=&quot;updateTime&quot; /&gt; &lt;result column=&quot;username&quot; property=&quot;username&quot; /&gt; &lt;/resultMap&gt; &lt;!-- 根据用户名查询用户--&gt; &lt;select id=&quot;loadUserByUsername&quot; resultMap=&quot;UserMap&quot;&gt; select * from t_user where username=#{username} &lt;/select&gt;&lt;/mapper&gt; 总结 这次的错误排查用了我将近一天的时间，对于新手来说，太折磨了 于是，写一个文档进行记录，防止下次再遇到同样的问题。 springboot启动失败的时候，若日志显示找不到什么文件，估计是配置的问题。 配置项目的时候最好再官网上查查稳定的版本。 在配置mybatis的时候，容易犯的错误： 1、要注意属性文件中，实体类地址和mapper地址是否正确 12type-aliases-package: com.mybatislearn.mockingjay.entitymapper-locations: classpath:mapper/*.xml 2、配置Mapper.xml的时候，注意标签下的namespace是否正确 1&lt;mapper namespace=&quot;com.mybatislearn.mockingjay.mapper.UserMapper&quot;&gt; 注意标签中的 type地址是否正确 1&lt;resultMap id=&quot;UserMap&quot; type=&quot;com.mybatislearn.mockingjay.entity.User&quot; &gt; 映射语句中的 id 是否正确对应映射接口的方法名 123&lt;select id=&quot;loadUserByUsername&quot; resultMap=&quot;UserMap&quot;&gt; select * from t_user where username=#{username}&lt;/select&gt; 123456package com.mybatislearn.mockingjay.mapper;import com.mybatislearn.mockingjay.entity.User;public interface UserMapper { User loadUserByUsername(String username);} 我拍的！好看不？哈哈~","link":"cn/%E8%AE%B0%E4%B8%80%E6%AC%A1Springboot+mybaits%E9%85%8D%E7%BD%AE%E7%BB%8F%E5%8E%86/"},{"title":"Java泛型","text":"java泛型简介 Oracle官网中对于java泛型的描述： https://docs.oracle.com/javase/tutorial/java/generics/why.html 泛型的用处：泛型使类型（类和接口）在定义类、接口和方法时成为参数，从而使相同的一段代码处理多种不同类型的传入数据 来自官网的例子，使用泛型的函数或类使用的形式： 123List&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(&quot;hello&quot;);String s = list.get(0); // no cast 而没有使用泛型时，需要强制转换： 123List list = new ArrayList();list.add(&quot;hello&quot;);String s = (String) list.get(0); java中的List接口，提供对不同类型的类的标准方法。想象一下，若没有java泛型，则只能使用强制转换，并带来某些问题。 对于其中的get()函数： 12345678910ArrayList.java ......public E get(int index) { Objects.checkIndex(index, size); return elementData(index);}...... 可见此函数直接返回容器内的类，不需要强制转换的步骤，程序更加安全。 泛型使程序员能够实现通用算法。 通过使用泛型，程序员可以实现适用于不同类型集合的泛型算法，安全且易于阅读 java泛型类 java创建泛型类的原因是为了容器类的实现。 容器类是用来保存对象的，将对象放入其中，并对放入的一个或多个对象进行操作，如排序，增加，删除，查找等。这就要求容器类需要接收不同类型的类。所以必须使用一个机制，使容器类对所有的类都来者不拒。泛型类应运而生。 java泛型类简介 举个例子，假设定义一个简单的类： 123456public class Box { private Object object; public void set(Object object) { this.object = object; } public Object get() { return object; }} 这样的类存在的问题：此类的功能时接收一个类和返回接收的类，所以可能导致接收的类和期望返回的类（可能使用强制转换）不是同一个类型，导致运行时错误。同时在编译时此类错误无法被识别。 于是，改写为java泛型类： 1234567891011/** * Generic version of the Box class. * @param &lt;T&gt; the type of the value being boxed */public class Box&lt;T&gt; { // T stands for &quot;Type&quot; private T t; public void set(T t) { this.t = t; } public T get() { return t; }} java泛型类保证了传入的类型和期望返回的类型是相同的。使用“public class Box”来创建泛型声明，这引入了类型变量 T，代表传入对象的类型。 java泛型类的定义 java泛型类使用如下格式定义： 1class name&lt;T1, T2, ..., Tn&gt; { /* ... */ } 类型参数部分由尖括号 (&lt;&gt;) 分隔，跟在类名之后。 它指定类型参数（也称为类型变量）T1、T2、…和 Tn。 在java的文档上有一个类型参数命名约定： E - Element K - Key N - Number T - Type V - Value S,U,V etc. - 2nd, 3rd, 4th types java泛型类使用 以Box泛型类为例，要从代码中引用泛型 Box 类，您必须执行泛型类型调用，它将 T 替换为一些具体值，例如 Integer： 1Box&lt;Integer&gt; integerBox; 可以将泛型类型调用视为类似于普通方法调用，但不是将参数传递给方法，而是将类型参数（在本例中为 Integer）传递给 Box 类本身 与任何其他变量声明一样，此代码实际上并未创建新的 Box 对象。 它只是声明 integerBox 将保存对“Box of Integer”的引用，这就是 Box 的读取方式。 泛型类型的调用通常称为参数化类型。 要实例化这个类，像往常一样使用 new 关键字，但将 放在类名和括号之间: 1Box&lt;Integer&gt; integerBox = new Box&lt;Integer&gt;();//泛型的实例化 java泛型函数 java泛型函数简介 和java泛型类类似，泛型函数通过引入所在类的类型参数。类型参数的范围仅限于声明它的方法。 允许使用静态和非静态泛型方法，以及泛型类构造函数。 java泛型函数定义 泛型方法的语法包括一个类型参数列表，在尖括号内,它出现在方法的返回类型之前，例如： 1public &lt;K, V&gt; boolean function(K k,V v) 对于静态泛型方法，类型参数部分必须出现在方法的返回类型之前,例如： 123456public class Util { public static &lt;K, V&gt; boolean compare(Pair&lt;K, V&gt; p1, Pair&lt;K, V&gt; p2) { return p1.getKey().equals(p2.getKey()) &amp;&amp; p1.getValue().equals(p2.getValue()); }} 泛型函数可以使用泛型类中的类型参数，例如： 123456789101112131415public class Pair&lt;K, V&gt; {//泛型类声明 private K key;//使用类型参数 private V value; public Pair(K key, V value) { this.key = key; this.value = value; } public void setKey(K key) { this.key = key; } public void setValue(V value) { this.value = value; } public K getKey() { return key; } public V getValue() { return value; }} 有界类型参数 有界类型参数简介 在使用java泛型的时候，可能会希望限制用作参数化类型中的类型参数的类型，也就是限制传入类的类型种类。譬如，只接受Number类型及其子类的对象作为实例化参数。这就是有界类型参数的用途。 有界类型参数的定义 形如： 1&lt;T extends Integer&gt; 称为有界类型参数 直接上示例代码： 1234567891011121314151617181920212223public class Box&lt;T&gt; { private T t; public void set(T t) { this.t = t; } public T get() { return t; } public &lt;U extends Number&gt; void inspect(U u){ //有界类型参数 System.out.println(&quot;T: &quot; + t.getClass().getName()); System.out.println(&quot;U: &quot; + u.getClass().getName()); } public static void main(String[] args) { Box&lt;Integer&gt; integerBox = new Box&lt;Integer&gt;(); integerBox.set(new Integer(10)); integerBox.inspect(&quot;some text&quot;); // error: this is still String! }} 上述代码在编译阶段会检查inspect函数中传输的参数类型，并给出错误提示，以保证传入的类型是Number及其子类。 同样也可以在java泛型类声明的时候使用有界类型参数，在编译此类型参数在类中的函数使用参数类型T的时候进行检查，保证类中所有使用T的函数都符合限定的类型： 123456789101112public class NaturalNumber&lt;T extends Integer&gt; { private T n; public NaturalNumber(T n) { this.n = n; } public boolean isEven() { return n.intValue() % 2 == 0; } // ...} 多重有界类型参数的定义 前面的示例说明了使用具有单个边界的类型参数，但类型参数可以具有多个边界 1&lt;T extends B1 &amp; B2 &amp; B3&gt; 具有多个边界的类型变量是边界中列出的所有类型的子类型。如果边界之一是类，则必须首先指定它 12345Class A { /* ... */ }interface B { /* ... */ }interface C { /* ... */ }class D &lt;T extends A &amp; B &amp; C&gt; { /* ... */ } 如果未首先指定绑定 A，则会出现编译时错误 1class D &lt;T extends B &amp; A &amp; C&gt; { /* ... */ } // compile-time error 泛型的继承和子类 如您所知，只要类型兼容，就可以将一种类型的对象分配给另一种类型的对象。 泛型也是如此。您可以执行泛型类型调用，将 Number 作为其类型参数传递，如果参数与 Number 兼容，则允许任何后续的 add 调用： 123Box&lt;Number&gt; box = new Box&lt;Number&gt;();box.add(new Integer(10)); // OKbox.add(new Double(10.1)); // OK 看看下面的示例 1public void boxTest(Box&lt;Number&gt; n) { /* ... */ } boxTest接受一个类型为Box的参数，但在使用中不能传入Box或者Box，因为不是Box的子类型。 所以可以有这样一张图： 给定两个具体类型 A 和 B（例如，Number 和 Integer）。 无论 A 和 B 是否相关，MyClass&lt;A&gt;与 MyClass&lt;B&gt; 都没有关系。 MyClass&lt;A&gt; 和 MyClass&lt;B&gt; 的共同父对象是 Object。 那么，java泛型类有怎样的继承关系呢： 以 Collections 类为例，ArrayList 实现了 List，而 List 扩展了 Collection。 所以 ArrayList 是 List 的子类型，List 是 Collection 的子类型。 只要不改变类型参数，类型之间的子类型关系就会保留。 若泛型类存在多个类型参数，也可以从但类型参数的泛型类中继承 1234interface PayloadList&lt;E,P&gt; extends List&lt;E&gt; { void setPayload(int index, P val); ...} PayloadList 的以下参数化是 List 的子类型 PayloadList&lt;String,String&gt; PayloadList&lt;String,Integer&gt; PayloadList&lt;String,Exception&gt; 它们的关系如图所示 java通配符 在java泛型中，称为通配符的问号 ( ? )表示未知类型。通配符可用于多种情况：作为参数、字段或局部变量的类型；有时作为返回类型。通配符永远不会用作泛型方法调用、泛型类实例创建或超类型的类型参数。 上界通配符 在Java编程中，可以使用上界通配符来放宽对类型参数的限制。 举一个例子 1public static void process(List&lt;? extends Foo&gt; list) { /* ... */ } 上界通配符，&lt;? extends Foo&gt; ，其中Foo是任何类型的，匹配Foo和任何Foo的子类。下列process方法代码可以遍历使用list容器的Foo对象或者Foo类的子类对象的List容器类： 12345public static void process(List&lt;? extends Foo&gt; list) { for (Foo elem : list) { // ... }} 无界通配符 无界通配符类型使用通配符 ( ? )指定，例如List&lt;?&gt;。这称为未知类型列表。在两种情况下，无界通配符是一种有用的方法： 如果您正在编写可以使用Object类中提供的功能实现的方法。 当代码不依赖于泛型类中的类型参数时。例如，List.size或List.clear。事实上，Class&lt;?&gt;之所以如此常用，是因为Class&lt;T&gt; 中的大多数方法都不依赖于T。 内部没有使用T或者其他类型参数的函数： 12345public static void printList(List&lt;?&gt; list) { for (Object elem: list) System.out.print(elem + &quot; &quot;); System.out.println();} 下界通配符 下界通配符将未知类型限制为特定类型或该类型的父类型 以下代码将数字 1 到 10 添加到列表的末尾，该方法适用于List&lt;Integer&gt;、List&lt;Number&gt;和List&lt;Object&gt; 以及任何可以保存Integer值的类： 12345public static void addNumbers(List&lt;? super Integer&gt; list) { for (int i = 1; i &lt;= 10; i++) { list.add(i); }} 有关于通配符的子类型 如泛型的继承和子类中所述，泛型类或接口之间的关系不仅仅因为它们的类型之间存在关系。但是可以使用通配符来创建通用类或接口之间的关系。 给定两个非泛型类 12class A { /* ... */ }class B extends A { /* ... */ } 编写下列代码是正确的 12B b = new B();A a = b; 此示例显示常规类的继承遵循此子类型规则：如果B扩展A ，则类B是类A的子类型。此规则不适用于泛型类型： 12List&lt;B&gt; lb = new ArrayList&lt;&gt;();List&lt;A&gt; la = lb; // compile-time error 上述代码可以看出List&lt;B&gt;并不是List&lt;A&gt;的子类型 于是在学习了java通配符后，有这么一种关系： List和List共同的父级是List&lt;?&gt;,而List和List之间没有关系。 为了在这些类之间创建关系，以便代码可以通过List&lt;Integer&gt;的元素访问Number的方法，请使用上限通配符： 12List&lt;? extends Integer&gt; intList = new ArrayList&lt;&gt;();List&lt;? extends Number&gt; numList = intList; // OK. List&lt;? extends Integer&gt; is a subtype of List&lt;? extends Number&gt; Integer是Number的子类型，而numList是Number对象的列表，所以现在intList（Integer对象的列表）和numList之间存在关系。下图显示了使用上限和下限通配符声明的几个List类之间的关系。 根据上述规则可以列出通用List类声明的层次结构 对泛型的限制 无法使用原始类型实例化泛型类型 无法创建类型参数的实例 不能声明类型为类型参数的静态字段 不能对参数化类型使用 Casts 或instanceof 无法创建参数化类型的数组 无法创建、捕获或抛出参数化类型的对象 无法将每个重载的形式参数类型擦除为相同原始类型的方法重载","link":"cn/java%E6%B3%9B%E5%9E%8B/"},{"title":"一个对象复制的小技巧","text":"在具体的开发中遇到特殊的对象复制需要，同时最好不要添加过多的代码。 有类A和类B，类A和类B的属性名是一样的，但是属性类型是不一样的。AB类中相同名称的属性是两个相似的类，同名属性的类中属性属性是相同的。 例如： 1234public class DtoT { private Bean bean; private List&lt;BeanT&gt; list;} 和 1234public class Dto { private Bean bean; private List&lt;Bean&gt; list;} 的关系。 同时BeanT类 和 Bean类是具有相同的属性名和属性类型。 123public class Bean { String text;} 123public class BeanT { String text;} 若需要对Dto进行复制，传统的方法需要对List中的属性进行逐级复制，List中的类如果还有List就还需要在加一层嵌套进行复制。 由于BeanUtils.copyProperties使用反射机制进行复制，复制来的List之中的对象类型就依旧是被复制的类属性的类型，需要for循环中嵌套for循转，同时使用BeanUtils.copyProperties进行转换和复制。 这样就会产生一大段和业务无关的代码。 但是需求只是为了复制属性。 于是有这样一种情况 使用fastJson 进行对象的复制和转换。 1DtoT target = JSON.parseObject(JSON.toJSONString(source),DtoT.class); 使用fastJson 就可以实现不同类属性的复制。 使用一行的代码就可以完成之前for嵌套的操作，简化代码同时降低出错概率","link":"cn/%E4%B8%80%E4%B8%AA%E5%B0%8F%E6%8A%80%E5%B7%A7/"},{"title":"实习的第一个需求","text":"向着美好的生活前进 需求： Hive分区模版的批量导入和导出，采用excel的形式 某个客户需要在进行大数据集成平台的创建时，需要进行大量分区模版的导入和导出。 实现： 使用easyexcel进行模版的批量导入和导出 导出需要获得要导出的数据资产Id，查询该用户需要可以获得的资产Id，两者做交集，获得可以导出的资产Id，再去数据库中通过资产Id查询模版，同时也要查询没有模版，但有hive分区的资产。难度点在于批量查询数据时的数据交并处理，通过Map做关联，同时要注意批量查询后符合条件的数据不存在导致的空指针的问题。 分区模版的导入,使用easyexcel进行excel的解析，需要对excel内的每行每列进行数据的处理和分析 需要处理数据的有效性 处理分区字段的正确性 检验分区路径的权限 最后将检验结果写入redis做缓存 分区模版导入最后有一个确认导入的接口 使用redis内存入的分区数据 检验传入的需要导入的分区数据库名表名 进行数据的写入 生成执行分区的sql，并执行hive分区 名词： 分区模版：为了实现hive分区而做的一个模版，记录了hive分区的路径，分区的字段和分区字段的类型，以及创建新分区的频率和重试时间等信息 公司的工作方式 人多，则必乱。 想起来在实验室准备比赛的时候，基本上由两个主要的管理着管着下面十几个管理的人。信息同步全靠qq，请假啥的全靠私聊，办公室也就是两隔壁。一旦人多，多到二三十人，信息的同步就不能仅靠qq来实现了。这次实习算是见识到了千人级别的公司对信息同步的方法，不过日常的通讯还是腾讯的软件hhh。 一般的工作流程 接到一个需求之后，产品经理会去制定需求分析，然后设计大致初一个设计图，然后交给相关的开发负责人进行开发，开发完成后通知测试进行提测，测试通过后交于产品进行验收。 第一次做需求的感想 想好再开始写代码 在进行分区模版导出的时候，因为这个功能比较简单，所以直接上手写代码，并没有遇到特别的逻辑上的问题，结果就是我写的代码非常的冗长和多余。在之后的维护中，就极度的累。感觉自己给自己挖了一个坑差点就把自己埋进去。在之后我所在的组中负责相关功能的人看了我的代码后，觉得我的代码实在是有点太多了，甚至看着有些累赘。 在公司里和在学校实验室里不一样的结果导向 在学校的实验室里，我从来都可以不用担心我代码有什么奇怪的问题，而且在写代码的时候，也可以不考虑后期的维护等问题，实验室里的每一期代码都是专人负责到底，直到比赛结束。所有的代码都仅仅是为了实现一个功能，目标也就是为了使功能运行更加的完备。 公司里不是这样。 在公司里写代码，需要考虑代码的可维护性，代码的可阅读性，要考虑未来功能修改的便捷性，同时注解也不能乱写，是存在一定的规范的。同时，由于我接到的需求是对已经有的功能做批量处理，所以我所做的工作只是将已经有的功能做批量话处理。最好是使用原先的逻辑并做一个封装。不过我也是第一次写公司里的代码，没想好就上手写了，也没有考虑做功能的分割，只是一股脑的把想到的代码塞进去。结果就是臃肿冗余的结构，同时给我修bug的时候造成了许多麻烦。 方法和方向都很重要 带我的学长给我列了一个流程，一个写代码的基本流程： 写代码前先写出伪代码 根据伪代码一步步写出源码 将伪代码写入注释中 在我回忆起我的写代码的时候，确实发现自己操之过急，可能是实习中第一次写代码，草草上手，并没有做详细的分析，甚至出现了写着写着忘记思路的情况。在下一次写代码的时候，还是要对代码逻辑做一些分析和研究，写出伪代码作为依据，这样才可以将思路尽可能的保留下来。 需求需求需求！！！ 在写代码的时候，一定要充分了解需求是什么。需求是一切的基石。开发人员一旦误解了需求的含义，就会误判工作时长，拖慢整个组的进度，同时也会给自己添加麻烦。 总之 新手写代码，越写越慌，老手写代码不急不忙。","link":"cn/%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%81%9A%E9%9C%80%E6%B1%82/"},{"title":"三周打工人生活","text":"体验三周的打工人生活，以及打工人的焦虑和期待 一、公司 我实习的公司，蓝蓝的，logo是一条鲸鱼，在我入职的那天，在公司的14楼，HR部门给我发了一只鲸鱼的布偶，小小个，可以用手掌握住的那种小，特别的小，特别的精致，还有那么一丝的可爱气息。 我是在上班之前几天来到的杭州，一想到自己住的地方离西湖只有两个街区，就有一种心痒痒的小兴奋。再想到公司实习是双休，也就意味着我每周有两天的时间去我向往的西湖玩，促使我在来杭州的第二天就去了西湖。那天的西湖，人很多很热闹，阳光也正正好，湖面倒映太阳，波光粼粼。 那天下午本来是要去熟悉从住家到公司的路的，还有点时间，就去了西湖转了转。 第二天，正式去上班了，签了各种合同，两年的竞业协议，来到了办公桌前。 办公桌…emmm很脏，可能好久都没有人用了，灰尘多，还有些垃圾。在清理了之后，发现我的椅子的靠背是坏的，没法支撑，一靠就倒的那种。而发给我的电脑是一台13年的mac mini。有一点点卡，但是总体来说，只要系统不升级，那用着应该没什么问题，毕竟苹果对系统的调教水平摆在那里，不流畅也会让我感觉流畅的。我之前以为大公司起码都会发一台macbook的，直到我看到我隔壁桌浩哥的电脑，键盘有好几个键都磨的字都看不见了，我第一回看见电脑的键盘可以如此身残志坚，但很奇怪的是，浩哥为啥不买一个键盘呢？ 我被分到了公司里的一个平台开发组，其中一个组员是公司帮带计划中带我的学长，好像去年才入职。我的直系上司，也就是组长，看着人应该不错，有时还叫我和我们组其他人一起去吃饭，好像我和组员一样都已经为公司做了大半年的事情了，公司提供的晚饭真的超级好，每个月有550的餐补，东西随便吃，就算加一个饮料也就30多块钱一餐。 好像自己突然有钱了一样，感觉可以放开了吃，而且，呵呵，不会再像学校里那样吃坏肚子。中午的话，公司不管，自己点外卖，美团上每家店都点一点，开一个美团会员，这样算下来还挺省的。哈哈，在也不用吃学校那难吃又拉肚子的饭了。 杭州太棒了，不过现在我没有房租的问题，住在亲戚家里就是好。 杭州太棒了，太好了，好想留在杭州，留在这家公司。不知道我的能力够不够的到。 二、焦虑 我总是会过度的焦虑，怕哪里做的不对，把事情和我想象中的不太一样。 在大一的时候，我曾经为了加入实验室，基本在打卡时都会提早个十几分钟，感觉迟到是一件天大的事情。甚至在某一天的中午，本来在好好睡觉的我不知道为什么看了眼手表，猛的惊醒了，匆匆忙整理东西就奔向了我的实验室——我把时间看错了，十分钟的路程我提早了四十分钟就出门了，在快到的时候才意识到，自己来的太早了。 但是我从前不是这样的人。我小学的时候，我父母最担心的事情就是我的成绩，真的太烂了，班里倒数，而我却满不在乎。在初中的时候，我莫名其妙的上了市里的重点中学又很不意外的成了班里的倒数第一。有些人从小就知道要认真学习，而有些人却是在很迟的时候才有所醒悟，而我就是那个很迟才醒悟的人。但问题是我是一个倔强的人，制作自己认为正确的事情，所以直到高中的时候我都是成绩不好的那一个，倔强的代价就是偏科的严重，但总算不是一个吊儿郎当的人。 来到大学之后，发现自己最想做的事还是在一个团队里面写代码。于是我就选择参加实验室，我从未发现我会如此喜欢一件事物，那么喜欢与优秀的人一起共事，甚至于害怕落选。 有人说，驱使人进步的有两种东西，一是人本身的欲望，二是对后果的恐惧。两者在我身上似乎都说的通，是对落选的害怕驱使我努力学习，同时对团队合作的向往也让我不断的催促自己向前。往往焦虑也就从这两个裂缝中钻出来了。在选拔的时候，我害怕在哪里做的不对，害怕在自己做的东西不够完美，甚至在学习的时候害怕自己学的不够快。就像刚刚实习的两周里一样。 实习开始的时候，带我的人还没有给我安排任务，这个时候我是处在一个非常清闲的状态，焦虑也就随之而来。在公司里，越是清闲，我也就越是焦虑。在配置完工作电脑的环境后，我就不知道应该做什么了，听带我的人说，我应该先对公司做的业务系统进行了解。但这种没有一个评价指标和没有一个时间限制的作业就会让我抓耳挠腮，我该了解到什么程度，我有多少时间了解，我会不会让带我的学长和组长失望。实习的第一二天，焦虑就蔓延全身。 还好在第三天，我收到学长给我制定的计划，我的工作线，和我的学习线都有了一个明确的标准和交付日期，也算是又了一个明确的目标，我的焦虑也降下来了一点。 三、羡慕 2021年刚过，2022年刚来就有人写一年的年终总结。这时候我看着别人写的总结就很羡慕，为什么别人的年中总结表达的就这么的流畅和通顺，字里行间就能感觉到当事人的情绪。我以前也是写年中总结的，但总差那么一点意思。说实话，我从小写作就属于完犊子类型的，基本就是一团浆糊。当语文老师真难，看到我这团浆糊哪怕是沾点酱油醋也要咽下去。在大学里，我发现我不再讨厌语文这门课，因为我发现在我看的一些视频制作者当中，有一些人可以脱口而出一些古诗诗词，就让人能瞬间明天他要表达的东西，这样的能力非常令我羡慕。就像有人会羡慕别人拍照好，唱歌好一样。 2022年我看到了我初中同学的年中总结，真的就可以在字里行间里看出一个人在一年之间的进步和心理变化，我也想写出这样的文章，哪怕没有人看，我写出来也会让我觉得心情愉悦。 我2022年不打算立什么flag，我把当前的事情做做好就好，大的计划一直都在，大到要好多年。 四、期望 第一个期望就是能在实习中得到组长和学长的认可，成功转正。 第二个是在技术上积累下几本书，消化并使用它 第三个在摄影的技术上进行学习，首先得在拿到第一个月的工资的时候买一台相机，拍一些vlog，拍一些满意的照片 第四个多写作，多表达，提高表达和写作能力","link":"cn/%E5%AE%9E%E4%B9%A0%E4%B8%A4%E5%91%A8%E8%AE%B0/"}],"tags":[{"name":"java","slug":"java","link":"tags/java/"},{"name":"Hive开发","slug":"Hive开发","link":"tags/Hive%E5%BC%80%E5%8F%91/"},{"name":"hugo","slug":"hugo","link":"tags/hugo/"},{"name":"go","slug":"go","link":"tags/go/"},{"name":"blog","slug":"blog","link":"tags/blog/"},{"name":"java学习","slug":"java学习","link":"tags/java%E5%AD%A6%E4%B9%A0/"},{"name":"springboot","slug":"springboot","link":"tags/springboot/"},{"name":"mybaits","slug":"mybaits","link":"tags/mybaits/"},{"name":"java开发","slug":"java开发","link":"tags/java%E5%BC%80%E5%8F%91/"},{"name":"随想","slug":"随想","link":"tags/%E9%9A%8F%E6%83%B3/"}],"categories":[{"name":"java","slug":"java","link":"categories/java/"},{"name":"Hive","slug":"Hive","link":"categories/Hive/"},{"name":"blog","slug":"blog","link":"categories/blog/"},{"name":"java并发","slug":"java并发","link":"categories/java%E5%B9%B6%E5%8F%91/"},{"name":"java开发","slug":"java开发","link":"categories/java%E5%BC%80%E5%8F%91/"},{"name":"随想","slug":"随想","link":"categories/%E9%9A%8F%E6%83%B3/"}]}